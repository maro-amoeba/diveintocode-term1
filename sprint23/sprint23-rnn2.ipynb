{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sprint23-rnn2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"mdcctgco2asH","colab_type":"text"},"cell_type":"markdown","source":["# RNNの説明"]},{"metadata":{"id":"WxnQuWtq-ast","colab_type":"text"},"cell_type":"markdown","source":["リカレントニューラルネットワーク（RNN）とは何なのかを簡潔に説明してください。  \n","\n","説明に含める事項  \n","\n","再帰的とはどういうことか  \n","これまでのニューラルネットワークとは何が違うのか  \n","どういったときに使われるのか  \n","___"]},{"metadata":{"id":"FB99Emhv_fdx","colab_type":"text"},"cell_type":"markdown","source":["RNNは再帰的に学習を行う経路があります。ループする経路があるということです。  \n","それによって「隠れ状態」を内部に記憶することができます  \n","\n","これまでのニューラルネットワークは、フィードフォワードと呼ばれるタイプのニューラルネット で、  \n","流れが一方向のネットワークのことを言います。  \n","循環はしていません。   \n","\n","RNNは時系列データに対応したニューラルネットワークです。  \n","データのシーケンシャルな要素が重要な場合、その順列を意識させた学習を行わせたいです。  \n","NLPで言えば、言葉の並びですね。  \n","「I say hello.」は正しい文だけれど、  \n","「hello say. I」といった順序はおかしい可能性が高いということが理解できるモデルを作ることができます。  \n","より実用的な話ですと、機械翻訳の性能向上に大きな貢献を果たしました。"]},{"metadata":{"id":"aJUwx-3JA7gq","colab_type":"text"},"cell_type":"markdown","source":["___\n","『周辺単語解説』  \n","\n","・隠れ状態 (hidden state)  \n","RNNの h は過去からの「状態」を記憶しています。ネットワークの「メモリ」です。  \n","一言で言えば、ある時刻(順列時)の状態を表しています。\n","\n","・BPTT  \n","Backpropagation Through Time  \n","時間方向に展開したニューラルネットワークの誤差逆伝播法\n","\n","・Truncated BPTT  \n","Truncatedとは「切り取った」という意味です。  \n","BPTTはそのままだと大きいデータでは長すぎるレイヤになってしまい、 \n","勾配消失や、計算量やメモリの観点で問題が起こるため、  \n","その長さを適当に切り取り「ブロック」として扱うことです\n","\n","・言語モデル  \n","単語の羅列に対して確率を算出します。  \n","それぞれの単語の事後確率の総乗より、同時確率を求めることで表されます。  \n","\n","・パープレキシティ  \n","「確率の逆数」を示しています。  \n","値が「1.25」や「5.0」のとき、直感的には「分岐数」と理解することができます。  \n","次に出現する候補の数が少ないほど(最高は1)良い結果と言えそうです。\n","___"]},{"metadata":{"id":"BI_y6XDr5MGN","colab_type":"text"},"cell_type":"markdown","source":["# LSTMの説明"]},{"metadata":{"id":"UtRWl8P35PWg","colab_type":"text"},"cell_type":"markdown","source":["LSTMとは何なのかを簡潔に説明してください。  \n","\n","説明に含める事項  \n","\n","SimpleRNNの持つ問題をどのように解決しているか  \n","ゲートと呼ばれる仕組みについて  \n","___"]},{"metadata":{"id":"15nPNKuB5bGA","colab_type":"text"},"cell_type":"markdown","source":["RNNに対して、「ゲート」という仕組みを導入したのがLSTMです。  \n","\n","SimpleRNNが持っていた問題とは、大きなデータセットを用いた時に  \n","BPTTで、勾配消失もしくは勾配爆発が起きてしまうことでした。  \n","\n","この問題を解決する方法として主に二つ手法を導入しており、  \n","それが、「勾配クリッピング」と「ゲート」です  \n","それぞれ、勾配爆発、勾配消失に対処しています。  \n","\n",">\n","\n","勾配クリッピングは単純な仕組みで、勾配のL2ノルムが一定値を超えたら  \n","((一定値)/勾配L2ノルム)*勾配 の計算をし、修正してあげる仕組みです。  \n","\n",">\n","\n","では、ゲートについてです。  \n","ゲートの役割は、「調整」です。イメージ的には  \n","水流の流れを調整する水門のような働きをしています。  \n","ゲートには3つの種類があり、  \n","「forgetゲート」「inputゲート」「outputゲート」があります  \n","ゲートには専用の重みがあり、sigmoid関数を用いて調整されています。  \n","これらによって、隠れ状態hの流れを調整します。  \n","\n",">\n","\n","さて、もう一つ重要なLSTMの特徴があります。記憶セルです。これはゲートにも関係してきます。\n","これはLSTMレイヤー専用の記憶部に相当します。  \n","記憶セルはLSTMレイヤー内だけでデータの受け渡しをしています。  \n","一方LSTMレイヤーの隠れ状態hは、他のレイヤーにも情報を流している違いがありますね。  \n","この記憶セルに、「forget」「input」ゲートを通った隠れ状態hの情報が流れます。  \n","重要なポイントは、この記憶セルの逆伝播は、「+」「×」の計算部分の通過のみで成り立ちます。  \n","「×」は「行列の積」ではなく、「アダマール積」である点も重要です。  \n","この仕組みによって、LSTMは勾配消失に対する問題も対応しています。  \n","\n","___"]},{"metadata":{"id":"o8gV9uM--BVJ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
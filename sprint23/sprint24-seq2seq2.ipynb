{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sprint24-seq2seq2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"OLBulEjxGYdc","colab_type":"text"},"cell_type":"markdown","source":["# Attention"]},{"metadata":{"id":"5bw8ztTuG1Z6","colab_type":"text"},"cell_type":"markdown","source":["前回のseq2seqを改善するためにAttentionと呼ばれる手法を導入します。  \n","SimpleRNNにゲートを加えてLSTMが作られたように、情報の流し方をより適切に制御することを目指します。"]},{"metadata":{"id":"Tn73fEpCG5Ax","colab_type":"text"},"cell_type":"markdown","source":["## Attentionとは"]},{"metadata":{"id":"rWlWtlj6G-h6","colab_type":"text"},"cell_type":"markdown","source":["まずは説明、理解が大事ですね。  \n","\n","翻訳や音声認識など、ある時系列データを別の時系列データに変換するタスクでは、  \n","時系列データ間に対応関係が存在することが多くありますが、  \n","Attentionではこの2つの時系列データ間の対応関係をデータから学習しています。  \n","\n","対応関係というのは「アライメント」とも表現されますが、  \n","例えば翻訳では、「猫=cat」のような翻訳先の単語と意味が対応していることを指します。  \n","つまり、Attentionは「入力と出力でどの単語が関連しているのか」を学習するのです。  \n","\n","Attentionの働きとしては「単語のアライメント抽出」と言えます。\n","\n","Attentionでは例えば、ベクトルの内積を使ってベクトル間の類似度を算出し、  \n","その類似度を用いた重み付き和ベクトルがAttentionの出力になります。  \n","\n","Attentionで使用する演算は微分可能であるため、バックプロパゲーションによって学習ができます。  \n","\n","Attentionが算出する重み(確率)を可視化することで、出入力の対応関係を見ることも出来ます。  \n","\n","進んだ話ではRNNの替わりにより改良されたAttentionを使用し、モデルの精度をあげているようです。  \n","それは Self-Attention と呼ばれる技術になります。"]},{"metadata":{"id":"EoRsUJucSkng","colab_type":"text"},"cell_type":"markdown","source":["## function"]},{"metadata":{"id":"02jL_gd3SoOH","colab_type":"code","colab":{}},"cell_type":"code","source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x - x.max(axis=1, keepdims=True)\n","        x = np.exp(x)\n","        x /= x.sum(axis=1, keepdims=True)\n","    elif x.ndim == 1:\n","        x = x - np.max(x)\n","        x = np.exp(x) / np.sum(np.exp(x))\n","\n","    return x\n","\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k_vGiyC0dr_q","colab_type":"text"},"cell_type":"markdown","source":["## optimizer"]},{"metadata":{"id":"XlmRLIf8dswP","colab_type":"code","colab":{}},"cell_type":"code","source":["class SGD:\n","    '''\n","    確率的勾配降下法（Stochastic Gradient Descent）\n","    '''\n","    def __init__(self, lr=0.01):\n","        self.lr = lr\n","        \n","    def update(self, params, grads):\n","        for i in range(len(params)):\n","            params[i] -= self.lr * grads[i]\n","\n","\n","class Momentum:\n","    '''\n","    Momentum SGD\n","    '''\n","    def __init__(self, lr=0.01, momentum=0.9):\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.v is None:\n","            self.v = []\n","            for param in params:\n","                self.v.append(np.zeros_like(param))\n","\n","        for i in range(len(params)):\n","            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n","            params[i] += self.v[i]\n","\n","\n","class Nesterov:\n","    '''\n","    Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\n","    '''\n","    def __init__(self, lr=0.01, momentum=0.9):\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.v is None:\n","            self.v = []\n","            for param in params:\n","                self.v.append(np.zeros_like(param))\n","\n","        for i in range(len(params)):\n","            self.v[i] *= self.momentum\n","            self.v[i] -= self.lr * grads[i]\n","            params[i] += self.momentum * self.momentum * self.v[i]\n","            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n","\n","\n","class AdaGrad:\n","    '''\n","    AdaGrad\n","    '''\n","    def __init__(self, lr=0.01):\n","        self.lr = lr\n","        self.h = None\n","        \n","    def update(self, params, grads):\n","        if self.h is None:\n","            self.h = []\n","            for param in params:\n","                self.h.append(np.zeros_like(param))\n","\n","        for i in range(len(params)):\n","            self.h[i] += grads[i] * grads[i]\n","            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n","\n","\n","class RMSprop:\n","    '''\n","    RMSprop\n","    '''\n","    def __init__(self, lr=0.01, decay_rate = 0.99):\n","        self.lr = lr\n","        self.decay_rate = decay_rate\n","        self.h = None\n","        \n","    def update(self, params, grads):\n","        if self.h is None:\n","            self.h = []\n","            for param in params:\n","                self.h.append(np.zeros_like(param))\n","\n","        for i in range(len(params)):\n","            self.h[i] *= self.decay_rate\n","            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n","            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n","\n","\n","class Adam:\n","    '''\n","    Adam (http://arxiv.org/abs/1412.6980v8)\n","    '''\n","    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n","        self.lr = lr\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.iter = 0\n","        self.m = None\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.m is None:\n","            self.m, self.v = [], []\n","            for param in params:\n","                self.m.append(np.zeros_like(param))\n","                self.v.append(np.zeros_like(param))\n","        \n","        self.iter += 1\n","        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n","\n","        for i in range(len(params)):\n","            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n","            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n","            \n","            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vpHm0KW2cE2q","colab_type":"text"},"cell_type":"markdown","source":["## util"]},{"metadata":{"id":"gPj3l4JhcGyi","colab_type":"code","colab":{}},"cell_type":"code","source":["def to_cpu(x):\n","    import numpy\n","    if type(x) == numpy.ndarray:\n","        return x\n","    return np.asnumpy(x)\n","\n","\n","def to_gpu(x):\n","    import cupy\n","    if type(x) == cupy.ndarray:\n","        return x\n","    return cupy.asarray(x)\n","\n","\n","def clip_grads(grads, max_norm):\n","    total_norm = 0\n","    for grad in grads:\n","        total_norm += np.sum(grad ** 2)\n","    total_norm = np.sqrt(total_norm)\n","\n","    rate = max_norm / (total_norm + 1e-6)\n","    if rate < 1:\n","        for grad in grads:\n","            grad *= rate\n","\n","\n","def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n","    print('evaluating perplexity ...')\n","    corpus_size = len(corpus)\n","    total_loss, loss_cnt = 0, 0\n","    max_iters = (corpus_size - 1) // (batch_size * time_size)\n","    jump = (corpus_size - 1) // batch_size\n","\n","    for iters in range(max_iters):\n","        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n","        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n","        time_offset = iters * time_size\n","        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n","        for t in range(time_size):\n","            for i, offset in enumerate(offsets):\n","                xs[i, t] = corpus[(offset + t) % corpus_size]\n","                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n","\n","        try:\n","            loss = model.forward(xs, ts, train_flg=False)\n","        except TypeError:\n","            loss = model.forward(xs, ts)\n","        total_loss += loss\n","\n","        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n","        sys.stdout.flush()\n","\n","    print('')\n","    ppl = np.exp(total_loss / max_iters)\n","    return ppl\n","\n","\n","def eval_seq2seq(model, question, correct, id_to_char,\n","                 verbos=False, is_reverse=False):\n","    ###################### add here\n","    import numpy as np\n","    ######################\n","    \n","    correct = correct.flatten()\n","    # 頭の区切り文字\n","    start_id = correct[0]\n","    correct = correct[1:]\n","    guess = model.generate(question, start_id, len(correct))\n","\n","    # 文字列へ変換\n","    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n","    correct = ''.join([id_to_char[int(c)] for c in correct])\n","    guess = ''.join([id_to_char[int(c)] for c in guess])\n","\n","    if verbos:\n","        if is_reverse:\n","            question = question[::-1]\n","\n","        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n","        print('Q', question)\n","        print('T', correct)\n","\n","        is_windows = os.name == 'nt'\n","\n","        if correct == guess:\n","            mark = colors['ok'] + '☑' + colors['close']\n","            if is_windows:\n","                mark = 'O'\n","            print(mark + ' ' + guess)\n","        else:\n","            mark = colors['fail'] + '☒' + colors['close']\n","            if is_windows:\n","                mark = 'X'\n","            print(mark + ' ' + guess)\n","        print('---')\n","\n","    return 1 if guess == correct else 0\n","\n","\n","def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n","    for word in (a, b, c):\n","        if word not in word_to_id:\n","            print('%s is not found' % word)\n","            return\n","\n","    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n","    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n","    query_vec = b_vec - a_vec + c_vec\n","    query_vec = normalize(query_vec)\n","\n","    similarity = np.dot(word_matrix, query_vec)\n","\n","    if answer is not None:\n","        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n","\n","    count = 0\n","    for i in (-1 * similarity).argsort():\n","        if np.isnan(similarity[i]):\n","            continue\n","        if id_to_word[i] in (a, b, c):\n","            continue\n","        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n","\n","        count += 1\n","        if count >= top:\n","            return\n","\n","\n","def normalize(x):\n","    if x.ndim == 2:\n","        s = np.sqrt((x * x).sum(1))\n","        x /= s.reshape((s.shape[0], 1))\n","    elif x.ndim == 1:\n","        s = np.sqrt((x * x).sum())\n","        x /= s\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TO5Ml7r5WbJj","colab_type":"text"},"cell_type":"markdown","source":["## layers"]},{"metadata":{"id":"sE7BvkBiWcxS","colab_type":"code","colab":{}},"cell_type":"code","source":["class MatMul:\n","    def __init__(self, W):\n","        self.params = [W]\n","        self.grads = [np.zeros_like(W)]\n","        self.x = None\n","\n","    def forward(self, x):\n","        W, = self.params\n","        out = np.dot(x, W)\n","        self.x = x\n","        return out\n","\n","    def backward(self, dout):\n","        W, = self.params\n","        dx = np.dot(dout, W.T)\n","        dW = np.dot(self.x.T, dout)\n","        self.grads[0][...] = dW\n","        return dx\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.params = [W, b]\n","        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n","        self.x = None\n","\n","    def forward(self, x):\n","        W, b = self.params\n","        out = np.dot(x, W) + b\n","        self.x = x\n","        return out\n","\n","    def backward(self, dout):\n","        W, b = self.params\n","        dx = np.dot(dout, W.T)\n","        dW = np.dot(self.x.T, dout)\n","        db = np.sum(dout, axis=0)\n","\n","        self.grads[0][...] = dW\n","        self.grads[1][...] = db\n","        return dx\n","\n","\n","class Softmax:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.out = None\n","\n","    def forward(self, x):\n","        self.out = softmax(x)\n","        return self.out\n","\n","    def backward(self, dout):\n","        dx = self.out * dout\n","        sumdx = np.sum(dx, axis=1, keepdims=True)\n","        dx -= self.out * sumdx\n","        return dx\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.y = None  # softmaxの出力\n","        self.t = None  # 教師ラベル\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","\n","        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n","        if self.t.size == self.y.size:\n","            self.t = self.t.argmax(axis=1)\n","\n","        loss = cross_entropy_error(self.y, self.t)\n","        return loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","\n","        dx = self.y.copy()\n","        dx[np.arange(batch_size), self.t] -= 1\n","        dx *= dout\n","        dx = dx / batch_size\n","\n","        return dx\n","\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = 1 / (1 + np.exp(-x))\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","        return dx\n","\n","\n","class SigmoidWithLoss:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.loss = None\n","        self.y = None  # sigmoidの出力\n","        self.t = None  # 教師データ\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = 1 / (1 + np.exp(-x))\n","\n","        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n","\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","\n","        dx = (self.y - self.t) * dout / batch_size\n","        return dx\n","\n","\n","class Dropout:\n","    '''\n","    http://arxiv.org/abs/1207.0580\n","    '''\n","    def __init__(self, dropout_ratio=0.5):\n","        self.params, self.grads = [], []\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","\n","    def forward(self, x, train_flg=True):\n","        if train_flg:\n","            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n","            return x * self.mask\n","        else:\n","            return x * (1.0 - self.dropout_ratio)\n","\n","    def backward(self, dout):\n","        return dout * self.mask\n","\n","\n","class Embedding:\n","    def __init__(self, W):\n","        self.params = [W]\n","        self.grads = [np.zeros_like(W)]\n","        self.idx = None\n","\n","    def forward(self, idx):\n","        W, = self.params\n","        self.idx = idx\n","        out = W[idx]\n","        return out\n","\n","    def backward(self, dout):\n","        dW, = self.grads\n","        dW[...] = 0\n","        np.add.at(dW, self.idx, dout)\n","        return None"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0bibjjeUWSyr","colab_type":"text"},"cell_type":"markdown","source":["## time_layers"]},{"metadata":{"id":"BXevW70SWP0T","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","class RNN:\n","    def __init__(self, Wx, Wh, b):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.cache = None\n","\n","    def forward(self, x, h_prev):\n","        Wx, Wh, b = self.params\n","        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n","        h_next = np.tanh(t)\n","\n","        self.cache = (x, h_prev, h_next)\n","        return h_next\n","\n","    def backward(self, dh_next):\n","        Wx, Wh, b = self.params\n","        x, h_prev, h_next = self.cache\n","\n","        dt = dh_next * (1 - h_next ** 2)\n","        db = np.sum(dt, axis=0)\n","        dWh = np.dot(h_prev.T, dt)\n","        dh_prev = np.dot(dt, Wh.T)\n","        dWx = np.dot(x.T, dt)\n","        dx = np.dot(dt, Wx.T)\n","\n","        self.grads[0][...] = dWx\n","        self.grads[1][...] = dWh\n","        self.grads[2][...] = db\n","\n","        return dx, dh_prev\n","\n","\n","class TimeRNN:\n","    def __init__(self, Wx, Wh, b, stateful=False):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.layers = None\n","\n","        self.h, self.dh = None, None\n","        self.stateful = stateful\n","\n","    def forward(self, xs):\n","        Wx, Wh, b = self.params\n","        N, T, D = xs.shape\n","        D, H = Wx.shape\n","\n","        self.layers = []\n","        hs = np.empty((N, T, H), dtype='f')\n","\n","        if not self.stateful or self.h is None:\n","            self.h = np.zeros((N, H), dtype='f')\n","\n","        for t in range(T):\n","            layer = RNN(*self.params)\n","            self.h = layer.forward(xs[:, t, :], self.h)\n","            hs[:, t, :] = self.h\n","            self.layers.append(layer)\n","\n","        return hs\n","\n","    def backward(self, dhs):\n","        Wx, Wh, b = self.params\n","        N, T, H = dhs.shape\n","        D, H = Wx.shape\n","\n","        dxs = np.empty((N, T, D), dtype='f')\n","        dh = 0\n","        grads = [0, 0, 0]\n","        for t in reversed(range(T)):\n","            layer = self.layers[t]\n","            dx, dh = layer.backward(dhs[:, t, :] + dh)\n","            dxs[:, t, :] = dx\n","\n","            for i, grad in enumerate(layer.grads):\n","                grads[i] += grad\n","\n","        for i, grad in enumerate(grads):\n","            self.grads[i][...] = grad\n","        self.dh = dh\n","\n","        return dxs\n","\n","    def set_state(self, h):\n","        self.h = h\n","\n","    def reset_state(self):\n","        self.h = None\n","\n","\n","class LSTM:\n","    def __init__(self, Wx, Wh, b):\n","        '''\n","        Parameters\n","        ----------\n","        Wx: 入力`x`用の重みパラーメタ（4つ分の重みをまとめる）\n","        Wh: 隠れ状態`h`用の重みパラメータ（4つ分の重みをまとめる）\n","        b: バイアス（4つ分のバイアスをまとめる）\n","        '''\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.cache = None\n","\n","    def forward(self, x, h_prev, c_prev):\n","        Wx, Wh, b = self.params\n","        N, H = h_prev.shape\n","\n","        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n","\n","        f = A[:, :H]\n","        g = A[:, H:2*H]\n","        i = A[:, 2*H:3*H]\n","        o = A[:, 3*H:]\n","\n","        f = sigmoid(f)\n","        g = np.tanh(g)\n","        i = sigmoid(i)\n","        o = sigmoid(o)\n","\n","        c_next = f * c_prev + g * i\n","        h_next = o * np.tanh(c_next)\n","\n","        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n","        return h_next, c_next\n","\n","    def backward(self, dh_next, dc_next):\n","        Wx, Wh, b = self.params\n","        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n","\n","        tanh_c_next = np.tanh(c_next)\n","\n","        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n","\n","        dc_prev = ds * f\n","\n","        di = ds * g\n","        df = ds * c_prev\n","        do = dh_next * tanh_c_next\n","        dg = ds * i\n","\n","        di *= i * (1 - i)\n","        df *= f * (1 - f)\n","        do *= o * (1 - o)\n","        dg *= (1 - g ** 2)\n","\n","        dA = np.hstack((df, dg, di, do))\n","\n","        dWh = np.dot(h_prev.T, dA)\n","        dWx = np.dot(x.T, dA)\n","        db = dA.sum(axis=0)\n","\n","        self.grads[0][...] = dWx\n","        self.grads[1][...] = dWh\n","        self.grads[2][...] = db\n","\n","        dx = np.dot(dA, Wx.T)\n","        dh_prev = np.dot(dA, Wh.T)\n","\n","        return dx, dh_prev, dc_prev\n","\n","\n","class TimeLSTM:\n","    def __init__(self, Wx, Wh, b, stateful=False):\n","        self.params = [Wx, Wh, b]\n","        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n","        self.layers = None\n","\n","        self.h, self.c = None, None\n","        self.dh = None\n","        self.stateful = stateful\n","\n","    def forward(self, xs):\n","        Wx, Wh, b = self.params\n","        N, T, D = xs.shape\n","        H = Wh.shape[0]\n","\n","        self.layers = []\n","        hs = np.empty((N, T, H), dtype='f')\n","\n","        if not self.stateful or self.h is None:\n","            self.h = np.zeros((N, H), dtype='f')\n","        if not self.stateful or self.c is None:\n","            self.c = np.zeros((N, H), dtype='f')\n","\n","        for t in range(T):\n","            layer = LSTM(*self.params)\n","            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n","            hs[:, t, :] = self.h\n","\n","            self.layers.append(layer)\n","\n","        return hs\n","\n","    def backward(self, dhs):\n","        Wx, Wh, b = self.params\n","        N, T, H = dhs.shape\n","        D = Wx.shape[0]\n","\n","        dxs = np.empty((N, T, D), dtype='f')\n","        dh, dc = 0, 0\n","\n","        grads = [0, 0, 0]\n","        for t in reversed(range(T)):\n","            layer = self.layers[t]\n","            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n","            dxs[:, t, :] = dx\n","            for i, grad in enumerate(layer.grads):\n","                grads[i] += grad\n","\n","        for i, grad in enumerate(grads):\n","            self.grads[i][...] = grad\n","        self.dh = dh\n","        return dxs\n","\n","    def set_state(self, h, c=None):\n","        self.h, self.c = h, c\n","\n","    def reset_state(self):\n","        self.h, self.c = None, None\n","\n","\n","class TimeEmbedding:\n","    def __init__(self, W):\n","        self.params = [W]\n","        self.grads = [np.zeros_like(W)]\n","        self.layers = None\n","        self.W = W\n","\n","    def forward(self, xs):\n","        N, T = xs.shape\n","        V, D = self.W.shape\n","\n","        out = np.empty((N, T, D), dtype='f')\n","        self.layers = []\n","\n","        for t in range(T):\n","            layer = Embedding(self.W)\n","            out[:, t, :] = layer.forward(xs[:, t])\n","            self.layers.append(layer)\n","\n","        return out\n","\n","    def backward(self, dout):\n","        N, T, D = dout.shape\n","\n","        grad = 0\n","        for t in range(T):\n","            layer = self.layers[t]\n","            layer.backward(dout[:, t, :])\n","            grad += layer.grads[0]\n","\n","        self.grads[0][...] = grad\n","        return None\n","\n","\n","class TimeAffine:\n","    def __init__(self, W, b):\n","        self.params = [W, b]\n","        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n","        self.x = None\n","\n","    def forward(self, x):\n","        N, T, D = x.shape\n","        W, b = self.params\n","\n","        rx = x.reshape(N*T, -1)\n","        out = np.dot(rx, W) + b\n","        self.x = x\n","        return out.reshape(N, T, -1)\n","\n","    def backward(self, dout):\n","        x = self.x\n","        N, T, D = x.shape\n","        W, b = self.params\n","\n","        dout = dout.reshape(N*T, -1)\n","        rx = x.reshape(N*T, -1)\n","\n","        db = np.sum(dout, axis=0)\n","        dW = np.dot(rx.T, dout)\n","        dx = np.dot(dout, W.T)\n","        dx = dx.reshape(*x.shape)\n","\n","        self.grads[0][...] = dW\n","        self.grads[1][...] = db\n","\n","        return dx\n","\n","\n","class TimeSoftmaxWithLoss:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.cache = None\n","        self.ignore_label = -1\n","\n","    def forward(self, xs, ts):\n","        N, T, V = xs.shape\n","\n","        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n","            ts = ts.argmax(axis=2)\n","\n","        mask = (ts != self.ignore_label)\n","\n","        # バッチ分と時系列分をまとめる（reshape）\n","        xs = xs.reshape(N * T, V)\n","        ts = ts.reshape(N * T)\n","        mask = mask.reshape(N * T)\n","\n","        ys = softmax(xs)\n","        ls = np.log(ys[np.arange(N * T), ts])\n","        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n","        loss = -np.sum(ls)\n","        loss /= mask.sum()\n","\n","        self.cache = (ts, ys, mask, (N, T, V))\n","        return loss\n","\n","    def backward(self, dout=1):\n","        ts, ys, mask, (N, T, V) = self.cache\n","\n","        dx = ys\n","        dx[np.arange(N * T), ts] -= 1\n","        dx *= dout\n","        dx /= mask.sum()\n","        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n","\n","        dx = dx.reshape((N, T, V))\n","\n","        return dx\n","\n","\n","class TimeDropout:\n","    def __init__(self, dropout_ratio=0.5):\n","        self.params, self.grads = [], []\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","        self.train_flg = True\n","\n","    def forward(self, xs):\n","        if self.train_flg:\n","            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n","            scale = 1 / (1.0 - self.dropout_ratio)\n","            self.mask = flg.astype(np.float32) * scale\n","\n","            return xs * self.mask\n","        else:\n","            return xs\n","\n","    def backward(self, dout):\n","        return dout * self.mask\n","\n","\n","class TimeBiLSTM:\n","    def __init__(self, Wx1, Wh1, b1,\n","                 Wx2, Wh2, b2, stateful=False):\n","        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n","        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n","        self.params = self.forward_lstm.params + self.backward_lstm.params\n","        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n","\n","    def forward(self, xs):\n","        o1 = self.forward_lstm.forward(xs)\n","        o2 = self.backward_lstm.forward(xs[:, ::-1])\n","        o2 = o2[:, ::-1]\n","\n","        out = np.concatenate((o1, o2), axis=2)\n","        return out\n","\n","    def backward(self, dhs):\n","        H = dhs.shape[2] // 2\n","        do1 = dhs[:, :, :H]\n","        do2 = dhs[:, :, H:]\n","\n","        dxs1 = self.forward_lstm.backward(do1)\n","        do2 = do2[:, ::-1]\n","        dxs2 = self.backward_lstm.backward(do2)\n","        dxs2 = dxs2[:, ::-1]\n","        dxs = dxs1 + dxs2\n","        return dxs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ddjrFBtebj16","colab_type":"text"},"cell_type":"markdown","source":["## BaseModel"]},{"metadata":{"id":"fHjAdI2dbr62","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","\n","class BaseModel:\n","    def __init__(self):\n","        self.params, self.grads = None, None\n","\n","    def forward(self, *args):\n","        raise NotImplementedError\n","\n","    def backward(self, *args):\n","        raise NotImplementedError\n","\n","    def save_params(self, file_name=None): # not use here notebook\n","        if file_name is None:\n","            file_name = self.__class__.__name__ + '.pkl'\n","\n","        params = [p.astype(np.float16) for p in self.params]\n","        if GPU:\n","            params = [to_cpu(p) for p in params]\n","\n","        with open(file_name, 'wb') as f:\n","            pickle.dump(params, f)\n","\n","    def load_params(self, file_name=None):\n","        if file_name is None:\n","            file_name = self.__class__.__name__ + '.pkl'\n","\n","        if '/' in file_name:\n","            file_name = file_name.replace('/', os.sep)\n","\n","        if not os.path.exists(file_name):\n","            raise IOError('No file: ' + file_name)\n","\n","        with open(file_name, 'rb') as f:\n","            params = pickle.load(f)\n","\n","        params = [p.astype('f') for p in params]\n","        if GPU:\n","            params = [to_gpu(p) for p in params]\n","\n","        for i, param in enumerate(self.params):\n","            param[...] = params[i]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1RdtYnBQSW22","colab_type":"text"},"cell_type":"markdown","source":["## seq2seq"]},{"metadata":{"id":"EXtRXqNSRnXd","colab_type":"code","colab":{}},"cell_type":"code","source":["class Encoder:\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        V, D, H = vocab_size, wordvec_size, hidden_size\n","        rn = np.random.randn\n","\n","        embed_W = (rn(V, D) / 100).astype('f')\n","        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n","        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n","        lstm_b = np.zeros(4 * H).astype('f')\n","\n","        self.embed = TimeEmbedding(embed_W)\n","        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n","\n","        self.params = self.embed.params + self.lstm.params\n","        self.grads = self.embed.grads + self.lstm.grads\n","        self.hs = None\n","\n","    def forward(self, xs):\n","        xs = self.embed.forward(xs)\n","        hs = self.lstm.forward(xs)\n","        self.hs = hs\n","        return hs[:, -1, :]\n","\n","    def backward(self, dh):\n","        dhs = np.zeros_like(self.hs)\n","        dhs[:, -1, :] = dh\n","\n","        dout = self.lstm.backward(dhs)\n","        dout = self.embed.backward(dout)\n","        return dout\n","\n","\n","class Decoder:\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        V, D, H = vocab_size, wordvec_size, hidden_size\n","        rn = np.random.randn\n","\n","        embed_W = (rn(V, D) / 100).astype('f')\n","        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n","        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n","        lstm_b = np.zeros(4 * H).astype('f')\n","        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n","        affine_b = np.zeros(V).astype('f')\n","\n","        self.embed = TimeEmbedding(embed_W)\n","        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n","        self.affine = TimeAffine(affine_W, affine_b)\n","\n","        self.params, self.grads = [], []\n","        for layer in (self.embed, self.lstm, self.affine):\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","    def forward(self, xs, h):\n","        self.lstm.set_state(h)\n","\n","        out = self.embed.forward(xs)\n","        out = self.lstm.forward(out)\n","        score = self.affine.forward(out)\n","        return score\n","\n","    def backward(self, dscore):\n","        dout = self.affine.backward(dscore)\n","        dout = self.lstm.backward(dout)\n","        dout = self.embed.backward(dout)\n","        dh = self.lstm.dh\n","        return dh\n","\n","    def generate(self, h, start_id, sample_size):\n","        sampled = []\n","        sample_id = start_id\n","        self.lstm.set_state(h)\n","\n","        for _ in range(sample_size):\n","            x = np.array(sample_id).reshape((1, 1))\n","            out = self.embed.forward(x)\n","            out = self.lstm.forward(out)\n","            score = self.affine.forward(out)\n","\n","            sample_id = np.argmax(score.flatten())\n","            sampled.append(int(sample_id))\n","\n","        return sampled\n","\n","\n","class Seq2seq(BaseModel):\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        V, D, H = vocab_size, wordvec_size, hidden_size\n","        self.encoder = Encoder(V, D, H)\n","        self.decoder = Decoder(V, D, H)\n","        self.softmax = TimeSoftmaxWithLoss()\n","\n","        self.params = self.encoder.params + self.decoder.params\n","        self.grads = self.encoder.grads + self.decoder.grads\n","\n","    def forward(self, xs, ts):\n","        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n","\n","        h = self.encoder.forward(xs)\n","        score = self.decoder.forward(decoder_xs, h)\n","        loss = self.softmax.forward(score, decoder_ts)\n","        return loss\n","\n","    def backward(self, dout=1):\n","        dout = self.softmax.backward(dout)\n","        dh = self.decoder.backward(dout)\n","        dout = self.encoder.backward(dh)\n","        return dout\n","\n","    def generate(self, xs, start_id, sample_size):\n","        h = self.encoder.forward(xs)\n","        sampled = self.decoder.generate(h, start_id, sample_size)\n","        return sampled"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h20TvwlxZavB","colab_type":"text"},"cell_type":"markdown","source":["## peeky_seq2seq"]},{"metadata":{"id":"vJhTuvsDZb5F","colab_type":"code","colab":{}},"cell_type":"code","source":["class PeekyDecoder:\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        V, D, H = vocab_size, wordvec_size, hidden_size\n","        rn = np.random.randn\n","\n","        embed_W = (rn(V, D) / 100).astype('f')\n","        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n","        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n","        lstm_b = np.zeros(4 * H).astype('f')\n","        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n","        affine_b = np.zeros(V).astype('f')\n","\n","        self.embed = TimeEmbedding(embed_W)\n","        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n","        self.affine = TimeAffine(affine_W, affine_b)\n","\n","        self.params, self.grads = [], []\n","        for layer in (self.embed, self.lstm, self.affine):\n","            self.params += layer.params\n","            self.grads += layer.grads\n","        self.cache = None\n","\n","    def forward(self, xs, h):\n","        N, T = xs.shape\n","        N, H = h.shape\n","\n","        self.lstm.set_state(h)\n","\n","        out = self.embed.forward(xs)\n","        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n","        out = np.concatenate((hs, out), axis=2)\n","\n","        out = self.lstm.forward(out)\n","        out = np.concatenate((hs, out), axis=2)\n","\n","        score = self.affine.forward(out)\n","        self.cache = H\n","        return score\n","\n","    def backward(self, dscore):\n","        H = self.cache\n","\n","        dout = self.affine.backward(dscore)\n","        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n","        dout = self.lstm.backward(dout)\n","        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n","        self.embed.backward(dembed)\n","\n","        dhs = dhs0 + dhs1\n","        dh = self.lstm.dh + np.sum(dhs, axis=1)\n","        return dh\n","\n","    def generate(self, h, start_id, sample_size):\n","        sampled = []\n","        char_id = start_id\n","        self.lstm.set_state(h)\n","\n","        H = h.shape[1]\n","        peeky_h = h.reshape(1, 1, H)\n","        for _ in range(sample_size):\n","            x = np.array([char_id]).reshape((1, 1))\n","            out = self.embed.forward(x)\n","\n","            out = np.concatenate((peeky_h, out), axis=2)\n","            out = self.lstm.forward(out)\n","            out = np.concatenate((peeky_h, out), axis=2)\n","            score = self.affine.forward(out)\n","\n","            char_id = np.argmax(score.flatten())\n","            sampled.append(char_id)\n","\n","        return sampled\n","\n","\n","class PeekySeq2seq(Seq2seq):\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        V, D, H = vocab_size, wordvec_size, hidden_size\n","        self.encoder = Encoder(V, D, H)\n","        self.decoder = PeekyDecoder(V, D, H)\n","        self.softmax = TimeSoftmaxWithLoss()\n","\n","        self.params = self.encoder.params + self.decoder.params\n","        self.grads = self.encoder.grads + self.decoder.grads"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tjx4tg5OdUDG","colab_type":"text"},"cell_type":"markdown","source":["## dataset"]},{"metadata":{"id":"49Yn8xW5dVRC","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import numpy\n","\n","\n","id_to_char = {}\n","char_to_id = {}\n","\n","\n","def _update_vocab(txt):\n","    chars = list(txt)\n","\n","    for i, char in enumerate(chars):\n","        if char not in char_to_id:\n","            tmp_id = len(char_to_id)\n","            char_to_id[char] = tmp_id\n","            id_to_char[tmp_id] = char\n","\n","\n","def load_data(file_name='date.txt', seed=1984):\n","    file_path = os.path.dirname(os.path.abspath('~/')) + '/' + file_name\n","\n","    if not os.path.exists(file_path):\n","        print('No file: %s' % file_name)\n","        return None\n","\n","    questions, answers = [], []\n","\n","    for line in open(file_path, 'r'):\n","        idx = line.find('_')\n","        questions.append(line[:idx])\n","        answers.append(line[idx:-1])\n","\n","    # create vocab dict\n","    for i in range(len(questions)):\n","        q, a = questions[i], answers[i]\n","        _update_vocab(q)\n","        _update_vocab(a)\n","\n","    # create numpy array\n","    x = numpy.zeros((len(questions), len(questions[0])), dtype=numpy.int)\n","    t = numpy.zeros((len(questions), len(answers[0])), dtype=numpy.int)\n","\n","    for i, sentence in enumerate(questions):\n","        x[i] = [char_to_id[c] for c in list(sentence)]\n","    for i, sentence in enumerate(answers):\n","        t[i] = [char_to_id[c] for c in list(sentence)]\n","\n","    # shuffle\n","    indices = numpy.arange(len(x))\n","    if seed is not None:\n","        numpy.random.seed(seed)\n","    numpy.random.shuffle(indices)\n","    x = x[indices]\n","    t = t[indices]\n","\n","    # 10% for validation set\n","    split_at = len(x) - len(x) // 10\n","    (x_train, x_test) = x[:split_at], x[split_at:]\n","    (t_train, t_test) = t[:split_at], t[split_at:]\n","\n","    return (x_train, t_train), (x_test, t_test)\n","\n","\n","def get_vocab():\n","    return char_to_id, id_to_char"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mj53wsmMcowD","colab_type":"text"},"cell_type":"markdown","source":["## Trainer"]},{"metadata":{"id":"6oy83X5reDbp","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import numpy\n","import time\n","import matplotlib.pyplot as plt\n","\n","\n","class Trainer:\n","    def __init__(self, model, optimizer):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.loss_list = []\n","        self.eval_interval = None\n","        self.current_epoch = 0\n","\n","    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n","        data_size = len(x)\n","        max_iters = data_size // batch_size\n","        self.eval_interval = eval_interval\n","        model, optimizer = self.model, self.optimizer\n","        total_loss = 0\n","        loss_count = 0\n","\n","        start_time = time.time()\n","        for epoch in range(max_epoch):\n","            # シャッフル\n","            idx = numpy.random.permutation(numpy.arange(data_size))\n","            x = x[idx]\n","            t = t[idx]\n","\n","            for iters in range(max_iters):\n","                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n","                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n","\n","                #################### add here\n","                if GPU:\n","                    batch_x = to_gpu(batch_x)\n","                    batch_t = to_gpu(batch_t)\n","                ####################\n","\n","                # 勾配を求め、パラメータを更新\n","                loss = model.forward(batch_x, batch_t)\n","                model.backward()\n","                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n","                if max_grad is not None:\n","                    clip_grads(grads, max_grad)\n","                optimizer.update(params, grads)\n","                total_loss += loss\n","                loss_count += 1\n","\n","                # 評価\n","                if (eval_interval is not None) and (iters % eval_interval) == 0:\n","                    avg_loss = total_loss / loss_count\n","                    elapsed_time = time.time() - start_time\n","                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n","                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n","                    self.loss_list.append(float(avg_loss))\n","                    total_loss, loss_count = 0, 0\n","\n","            self.current_epoch += 1\n","\n","    def plot(self, ylim=None):\n","        x = numpy.arange(len(self.loss_list))\n","        if ylim is not None:\n","            plt.ylim(*ylim)\n","        plt.plot(x, self.loss_list, label='train')\n","        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n","        plt.ylabel('loss')\n","        plt.show()\n","\n","\n","class RnnlmTrainer:\n","    def __init__(self, model, optimizer):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.time_idx = None\n","        self.ppl_list = None\n","        self.eval_interval = None\n","        self.current_epoch = 0\n","\n","    def get_batch(self, x, t, batch_size, time_size):\n","        batch_x = np.empty((batch_size, time_size), dtype='i')\n","        batch_t = np.empty((batch_size, time_size), dtype='i')\n","\n","        data_size = len(x)\n","        jump = data_size // batch_size\n","        offsets = [i * jump for i in range(batch_size)]  # バッチの各サンプルの読み込み開始位置\n","\n","        for time in range(time_size):\n","            for i, offset in enumerate(offsets):\n","                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n","                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n","            self.time_idx += 1\n","        return batch_x, batch_t\n","\n","    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n","            max_grad=None, eval_interval=20):\n","        data_size = len(xs)\n","        max_iters = data_size // (batch_size * time_size)\n","        self.time_idx = 0\n","        self.ppl_list = []\n","        self.eval_interval = eval_interval\n","        model, optimizer = self.model, self.optimizer\n","        total_loss = 0\n","        loss_count = 0\n","\n","        start_time = time.time()\n","        for epoch in range(max_epoch):\n","            for iters in range(max_iters):\n","                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n","\n","                # 勾配を求め、パラメータを更新\n","                loss = model.forward(batch_x, batch_t)\n","                model.backward()\n","                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n","                if max_grad is not None:\n","                    clip_grads(grads, max_grad)\n","                optimizer.update(params, grads)\n","                total_loss += loss\n","                loss_count += 1\n","\n","                # パープレキシティの評価\n","                if (eval_interval is not None) and (iters % eval_interval) == 0:\n","                    ppl = np.exp(total_loss / loss_count)\n","                    elapsed_time = time.time() - start_time\n","                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n","                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n","                    self.ppl_list.append(float(ppl))\n","                    total_loss, loss_count = 0, 0\n","\n","            self.current_epoch += 1\n","\n","    def plot(self, ylim=None):\n","        x = numpy.arange(len(self.ppl_list))\n","        if ylim is not None:\n","            plt.ylim(*ylim)\n","        plt.plot(x, self.ppl_list, label='train')\n","        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n","        plt.ylabel('perplexity')\n","        plt.show()\n","\n","\n","def remove_duplicate(params, grads):\n","    '''\n","    パラメータ配列中の重複する重みをひとつに集約し、\n","    その重みに対応する勾配を加算する\n","    '''\n","    params, grads = params[:], grads[:]  # copy list\n","\n","    while True:\n","        find_flg = False\n","        L = len(params)\n","\n","        for i in range(0, L - 1):\n","            for j in range(i + 1, L):\n","                # 重みを共有する場合\n","                if params[i] is params[j]:\n","                    grads[i] += grads[j]  # 勾配の加算\n","                    find_flg = True\n","                    params.pop(j)\n","                    grads.pop(j)\n","                # 転置行列として重みを共有する場合（weight tying）\n","                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n","                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n","                    grads[i] += grads[j].T\n","                    find_flg = True\n","                    params.pop(j)\n","                    grads.pop(j)\n","\n","                if find_flg: break\n","            if find_flg: break\n","\n","        if not find_flg: break\n","\n","    return params, grads"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pVgQQQrkYvrw","colab_type":"text"},"cell_type":"markdown","source":["## attention_layer"]},{"metadata":{"id":"BL9L6Tg1Yz1c","colab_type":"code","colab":{}},"cell_type":"code","source":["class WeightSum:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.cache = None\n","\n","    def forward(self, hs, a):\n","        N, T, H = hs.shape\n","\n","        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n","        t = hs * ar\n","        c = np.sum(t, axis=1)\n","\n","        self.cache = (hs, ar)\n","        return c\n","\n","    def backward(self, dc):\n","        hs, ar = self.cache\n","        N, T, H = hs.shape\n","        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n","        dar = dt * hs\n","        dhs = dt * ar\n","        da = np.sum(dar, axis=2)\n","\n","        return dhs, da\n","\n","\n","class AttentionWeight:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.softmax = Softmax()\n","        self.cache = None\n","\n","    def forward(self, hs, h):\n","        N, T, H = hs.shape\n","\n","        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n","        t = hs * hr\n","        s = np.sum(t, axis=2)\n","        a = self.softmax.forward(s)\n","\n","        self.cache = (hs, hr)\n","        return a\n","\n","    def backward(self, da):\n","        hs, hr = self.cache\n","        N, T, H = hs.shape\n","\n","        ds = self.softmax.backward(da)\n","        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n","        dhs = dt * hr\n","        dhr = dt * hs\n","        dh = np.sum(dhr, axis=1)\n","\n","        return dhs, dh\n","\n","\n","class Attention:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.attention_weight_layer = AttentionWeight()\n","        self.weight_sum_layer = WeightSum()\n","        self.attention_weight = None\n","\n","    def forward(self, hs, h):\n","        a = self.attention_weight_layer.forward(hs, h)\n","        out = self.weight_sum_layer.forward(hs, a)\n","        self.attention_weight = a\n","        return out\n","\n","    def backward(self, dout):\n","        dhs0, da = self.weight_sum_layer.backward(dout)\n","        dhs1, dh = self.attention_weight_layer.backward(da)\n","        dhs = dhs0 + dhs1\n","        return dhs, dh\n","\n","\n","class TimeAttention:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.layers = None\n","        self.attention_weights = None\n","\n","    def forward(self, hs_enc, hs_dec):\n","        N, T, H = hs_dec.shape\n","        out = np.empty_like(hs_dec)\n","        self.layers = []\n","        self.attention_weights = []\n","\n","        for t in range(T):\n","            layer = Attention()\n","            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n","            self.layers.append(layer)\n","            self.attention_weights.append(layer.attention_weight)\n","\n","        return out\n","\n","    def backward(self, dout):\n","        N, T, H = dout.shape\n","        dhs_enc = 0\n","        dhs_dec = np.empty_like(dout)\n","\n","        for t in range(T):\n","            layer = self.layers[t]\n","            dhs, dh = layer.backward(dout[:, t, :])\n","            dhs_enc += dhs\n","            dhs_dec[:,t,:] = dh\n","\n","        return dhs_enc, dhs_dec"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kX5eTc2TYcYy","colab_type":"text"},"cell_type":"markdown","source":["## attention_seq2seq"]},{"metadata":{"id":"_Dnb5pL1YfkI","colab_type":"code","colab":{}},"cell_type":"code","source":["class AttentionEncoder(Encoder):\n","    def forward(self, xs):\n","        xs = self.embed.forward(xs)\n","        hs = self.lstm.forward(xs)\n","        return hs\n","\n","    def backward(self, dhs):\n","        dout = self.lstm.backward(dhs)\n","        dout = self.embed.backward(dout)\n","        return dout\n","\n","\n","class AttentionDecoder:\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        V, D, H = vocab_size, wordvec_size, hidden_size\n","        rn = np.random.randn\n","\n","        embed_W = (rn(V, D) / 100).astype('f')\n","        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n","        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n","        lstm_b = np.zeros(4 * H).astype('f')\n","        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n","        affine_b = np.zeros(V).astype('f')\n","\n","        self.embed = TimeEmbedding(embed_W)\n","        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n","        self.attention = TimeAttention()\n","        self.affine = TimeAffine(affine_W, affine_b)\n","        layers = [self.embed, self.lstm, self.attention, self.affine]\n","\n","        self.params, self.grads = [], []\n","        for layer in layers:\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","    def forward(self, xs, enc_hs):\n","        h = enc_hs[:,-1]\n","        self.lstm.set_state(h)\n","\n","        out = self.embed.forward(xs)\n","        dec_hs = self.lstm.forward(out)\n","        c = self.attention.forward(enc_hs, dec_hs)\n","        out = np.concatenate((c, dec_hs), axis=2)\n","        score = self.affine.forward(out)\n","\n","        return score\n","\n","    def backward(self, dscore):\n","        dout = self.affine.backward(dscore)\n","        N, T, H2 = dout.shape\n","        H = H2 // 2\n","\n","        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n","        denc_hs, ddec_hs1 = self.attention.backward(dc)\n","        ddec_hs = ddec_hs0 + ddec_hs1\n","        dout = self.lstm.backward(ddec_hs)\n","        dh = self.lstm.dh\n","        denc_hs[:, -1] += dh\n","        self.embed.backward(dout)\n","\n","        return denc_hs\n","\n","    def generate(self, enc_hs, start_id, sample_size):\n","        sampled = []\n","        sample_id = start_id\n","#         ################### add here\n","#         sample_id = to_cpu(sample_id)\n","#         ###################\n","        h = enc_hs[:, -1]\n","        self.lstm.set_state(h)\n","\n","        for _ in range(sample_size):\n","            \n","            x = np.array([sample_id]).reshape((1, 1))\n","\n","            out = self.embed.forward(x)\n","            dec_hs = self.lstm.forward(out)\n","            c = self.attention.forward(enc_hs, dec_hs)\n","            out = np.concatenate((c, dec_hs), axis=2)\n","            score = self.affine.forward(out)\n","\n","            sample_id = np.argmax(score.flatten())\n","            sampled.append(sample_id)\n","\n","        return sampled\n","\n","\n","class AttentionSeq2seq(Seq2seq):\n","    def __init__(self, vocab_size, wordvec_size, hidden_size):\n","        args = vocab_size, wordvec_size, hidden_size\n","        self.encoder = AttentionEncoder(*args)\n","        self.decoder = AttentionDecoder(*args)\n","        self.softmax = TimeSoftmaxWithLoss()\n","\n","        self.params = self.encoder.params + self.decoder.params\n","        self.grads = self.encoder.grads + self.decoder.grads"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tOXymq03ZBsR","colab_type":"text"},"cell_type":"markdown","source":["## train"]},{"metadata":{"id":"HDMfeU1DZCTu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":10391},"outputId":"09ad25d0-057c-47b5-d45f-4ec310a2f93c","executionInfo":{"status":"ok","timestamp":1538317377351,"user_tz":-540,"elapsed":24,"user":{"displayName":"maro amo","photoUrl":"","userId":"13741098860389575110"}}},"cell_type":"code","source":["import numpy as np\n","GPU = False\n","\n","# データの読み込み\n","(x_train, t_train), (x_test, t_test) = load_data('date.txt')\n","char_to_id, id_to_char = get_vocab()\n","\n","# 入力文を反転\n","x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n","\n","# ハイパーパラメータの設定\n","vocab_size = len(char_to_id)\n","wordvec_size = 16\n","hidden_size = 256\n","batch_size = 128\n","max_epoch = 10\n","max_grad = 5.0\n","\n","model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n","# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n","# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n","\n","optimizer = Adam()\n","trainer = Trainer(model, optimizer)\n","\n","acc_list = []\n","for epoch in range(max_epoch):\n","    trainer.fit(x_train, t_train, max_epoch=1,\n","                batch_size=batch_size, max_grad=max_grad)\n","\n","    correct_num = 0\n","    for i in range(len(x_test)):\n","        question, correct = x_test[[i]], t_test[[i]]\n","        verbose = i < 10\n","        correct_num += eval_seq2seq(model, question, correct,\n","                                    id_to_char, verbose, is_reverse=True)\n","\n","    acc = float(correct_num) / len(x_test)\n","    acc_list.append(acc)\n","    print('val acc %.3f%%' % (acc * 100))\n","\n","\n","model.save_params()\n","\n","# グラフの描画\n","x = np.arange(len(acc_list))\n","\n","################## add here\n","if GPU:\n","    x = to_cpu(x)\n","##################\n","\n","plt.plot(x, acc_list, marker='o')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.ylim(-0.05, 1.05)\n","plt.show()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n","| epoch 1 |  iter 21 / 351 | time 14[s] | loss 3.09\n","| epoch 1 |  iter 41 / 351 | time 28[s] | loss 1.90\n","| epoch 1 |  iter 61 / 351 | time 42[s] | loss 1.72\n","| epoch 1 |  iter 81 / 351 | time 56[s] | loss 1.46\n","| epoch 1 |  iter 101 / 351 | time 70[s] | loss 1.19\n","| epoch 1 |  iter 121 / 351 | time 84[s] | loss 1.14\n","| epoch 1 |  iter 141 / 351 | time 98[s] | loss 1.09\n","| epoch 1 |  iter 161 / 351 | time 113[s] | loss 1.06\n","| epoch 1 |  iter 181 / 351 | time 127[s] | loss 1.04\n","| epoch 1 |  iter 201 / 351 | time 141[s] | loss 1.03\n","| epoch 1 |  iter 221 / 351 | time 155[s] | loss 1.02\n","| epoch 1 |  iter 241 / 351 | time 169[s] | loss 1.02\n","| epoch 1 |  iter 261 / 351 | time 183[s] | loss 1.01\n","| epoch 1 |  iter 281 / 351 | time 197[s] | loss 1.00\n","| epoch 1 |  iter 301 / 351 | time 211[s] | loss 1.00\n","| epoch 1 |  iter 321 / 351 | time 226[s] | loss 1.00\n","| epoch 1 |  iter 341 / 351 | time 240[s] | loss 1.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[91m☒\u001b[0m 1978-08-11\n","---\n","val acc 0.000%\n","| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n","| epoch 2 |  iter 21 / 351 | time 14[s] | loss 1.00\n","| epoch 2 |  iter 41 / 351 | time 28[s] | loss 0.99\n","| epoch 2 |  iter 61 / 351 | time 42[s] | loss 0.99\n","| epoch 2 |  iter 81 / 351 | time 57[s] | loss 0.99\n","| epoch 2 |  iter 101 / 351 | time 71[s] | loss 0.99\n","| epoch 2 |  iter 121 / 351 | time 85[s] | loss 0.99\n","| epoch 2 |  iter 141 / 351 | time 99[s] | loss 0.98\n","| epoch 2 |  iter 161 / 351 | time 113[s] | loss 0.98\n","| epoch 2 |  iter 181 / 351 | time 127[s] | loss 0.97\n","| epoch 2 |  iter 201 / 351 | time 141[s] | loss 0.95\n","| epoch 2 |  iter 221 / 351 | time 155[s] | loss 0.94\n","| epoch 2 |  iter 241 / 351 | time 170[s] | loss 0.90\n","| epoch 2 |  iter 261 / 351 | time 184[s] | loss 0.83\n","| epoch 2 |  iter 281 / 351 | time 198[s] | loss 0.74\n","| epoch 2 |  iter 301 / 351 | time 212[s] | loss 0.66\n","| epoch 2 |  iter 321 / 351 | time 226[s] | loss 0.58\n","| epoch 2 |  iter 341 / 351 | time 241[s] | loss 0.46\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[91m☒\u001b[0m 2006-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[91m☒\u001b[0m 2007-08-09\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[91m☒\u001b[0m 1983-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[91m☒\u001b[0m 2016-11-08\n","---\n","val acc 51.640%\n","| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n","| epoch 3 |  iter 21 / 351 | time 15[s] | loss 0.30\n","| epoch 3 |  iter 41 / 351 | time 29[s] | loss 0.21\n","| epoch 3 |  iter 61 / 351 | time 43[s] | loss 0.14\n","| epoch 3 |  iter 81 / 351 | time 57[s] | loss 0.09\n","| epoch 3 |  iter 101 / 351 | time 72[s] | loss 0.07\n","| epoch 3 |  iter 121 / 351 | time 86[s] | loss 0.05\n","| epoch 3 |  iter 141 / 351 | time 100[s] | loss 0.04\n","| epoch 3 |  iter 161 / 351 | time 115[s] | loss 0.03\n","| epoch 3 |  iter 181 / 351 | time 129[s] | loss 0.03\n","| epoch 3 |  iter 201 / 351 | time 143[s] | loss 0.02\n","| epoch 3 |  iter 221 / 351 | time 158[s] | loss 0.02\n","| epoch 3 |  iter 241 / 351 | time 172[s] | loss 0.02\n","| epoch 3 |  iter 261 / 351 | time 187[s] | loss 0.01\n","| epoch 3 |  iter 281 / 351 | time 201[s] | loss 0.01\n","| epoch 3 |  iter 301 / 351 | time 215[s] | loss 0.01\n","| epoch 3 |  iter 321 / 351 | time 230[s] | loss 0.01\n","| epoch 3 |  iter 341 / 351 | time 244[s] | loss 0.01\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 99.900%\n","| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n","| epoch 4 |  iter 21 / 351 | time 15[s] | loss 0.01\n","| epoch 4 |  iter 41 / 351 | time 29[s] | loss 0.01\n","| epoch 4 |  iter 61 / 351 | time 43[s] | loss 0.01\n","| epoch 4 |  iter 81 / 351 | time 58[s] | loss 0.01\n","| epoch 4 |  iter 101 / 351 | time 72[s] | loss 0.01\n","| epoch 4 |  iter 121 / 351 | time 86[s] | loss 0.00\n","| epoch 4 |  iter 141 / 351 | time 101[s] | loss 0.01\n","| epoch 4 |  iter 161 / 351 | time 115[s] | loss 0.00\n","| epoch 4 |  iter 181 / 351 | time 129[s] | loss 0.00\n","| epoch 4 |  iter 201 / 351 | time 144[s] | loss 0.00\n","| epoch 4 |  iter 221 / 351 | time 158[s] | loss 0.00\n","| epoch 4 |  iter 241 / 351 | time 173[s] | loss 0.00\n","| epoch 4 |  iter 261 / 351 | time 187[s] | loss 0.00\n","| epoch 4 |  iter 281 / 351 | time 201[s] | loss 0.00\n","| epoch 4 |  iter 301 / 351 | time 216[s] | loss 0.00\n","| epoch 4 |  iter 321 / 351 | time 230[s] | loss 0.00\n","| epoch 4 |  iter 341 / 351 | time 244[s] | loss 0.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 99.900%\n","| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n","| epoch 5 |  iter 21 / 351 | time 15[s] | loss 0.00\n","| epoch 5 |  iter 41 / 351 | time 29[s] | loss 0.00\n","| epoch 5 |  iter 61 / 351 | time 43[s] | loss 0.00\n","| epoch 5 |  iter 81 / 351 | time 58[s] | loss 0.00\n","| epoch 5 |  iter 101 / 351 | time 72[s] | loss 0.00\n","| epoch 5 |  iter 121 / 351 | time 87[s] | loss 0.00\n","| epoch 5 |  iter 141 / 351 | time 101[s] | loss 0.00\n","| epoch 5 |  iter 161 / 351 | time 115[s] | loss 0.00\n","| epoch 5 |  iter 181 / 351 | time 130[s] | loss 0.00\n","| epoch 5 |  iter 201 / 351 | time 144[s] | loss 0.00\n","| epoch 5 |  iter 221 / 351 | time 159[s] | loss 0.00\n","| epoch 5 |  iter 241 / 351 | time 173[s] | loss 0.00\n","| epoch 5 |  iter 261 / 351 | time 188[s] | loss 0.00\n","| epoch 5 |  iter 281 / 351 | time 202[s] | loss 0.00\n","| epoch 5 |  iter 301 / 351 | time 216[s] | loss 0.00\n","| epoch 5 |  iter 321 / 351 | time 231[s] | loss 0.00\n","| epoch 5 |  iter 341 / 351 | time 245[s] | loss 0.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 99.920%\n","| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n","| epoch 6 |  iter 21 / 351 | time 15[s] | loss 0.00\n","| epoch 6 |  iter 41 / 351 | time 29[s] | loss 0.00\n","| epoch 6 |  iter 61 / 351 | time 43[s] | loss 0.00\n","| epoch 6 |  iter 81 / 351 | time 58[s] | loss 0.00\n","| epoch 6 |  iter 101 / 351 | time 72[s] | loss 0.00\n","| epoch 6 |  iter 121 / 351 | time 87[s] | loss 0.00\n","| epoch 6 |  iter 141 / 351 | time 102[s] | loss 0.00\n","| epoch 6 |  iter 161 / 351 | time 116[s] | loss 0.00\n","| epoch 6 |  iter 181 / 351 | time 130[s] | loss 0.00\n","| epoch 6 |  iter 201 / 351 | time 145[s] | loss 0.00\n","| epoch 6 |  iter 221 / 351 | time 159[s] | loss 0.00\n","| epoch 6 |  iter 241 / 351 | time 174[s] | loss 0.00\n","| epoch 6 |  iter 261 / 351 | time 188[s] | loss 0.00\n","| epoch 6 |  iter 281 / 351 | time 203[s] | loss 0.00\n","| epoch 6 |  iter 301 / 351 | time 217[s] | loss 0.00\n","| epoch 6 |  iter 321 / 351 | time 231[s] | loss 0.00\n","| epoch 6 |  iter 341 / 351 | time 246[s] | loss 0.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 99.940%\n","| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n","| epoch 7 |  iter 21 / 351 | time 15[s] | loss 0.00\n","| epoch 7 |  iter 41 / 351 | time 29[s] | loss 0.00\n","| epoch 7 |  iter 61 / 351 | time 44[s] | loss 0.00\n","| epoch 7 |  iter 81 / 351 | time 58[s] | loss 0.00\n","| epoch 7 |  iter 101 / 351 | time 73[s] | loss 0.00\n","| epoch 7 |  iter 121 / 351 | time 87[s] | loss 0.00\n","| epoch 7 |  iter 141 / 351 | time 101[s] | loss 0.00\n","| epoch 7 |  iter 161 / 351 | time 116[s] | loss 0.00\n","| epoch 7 |  iter 181 / 351 | time 130[s] | loss 0.00\n","| epoch 7 |  iter 201 / 351 | time 145[s] | loss 0.00\n","| epoch 7 |  iter 221 / 351 | time 159[s] | loss 0.00\n","| epoch 7 |  iter 241 / 351 | time 173[s] | loss 0.00\n","| epoch 7 |  iter 261 / 351 | time 188[s] | loss 0.00\n","| epoch 7 |  iter 281 / 351 | time 202[s] | loss 0.00\n","| epoch 7 |  iter 301 / 351 | time 217[s] | loss 0.02\n","| epoch 7 |  iter 321 / 351 | time 231[s] | loss 0.04\n","| epoch 7 |  iter 341 / 351 | time 245[s] | loss 0.01\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 99.180%\n","| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.01\n","| epoch 8 |  iter 21 / 351 | time 15[s] | loss 0.00\n","| epoch 8 |  iter 41 / 351 | time 29[s] | loss 0.00\n","| epoch 8 |  iter 61 / 351 | time 43[s] | loss 0.00\n","| epoch 8 |  iter 81 / 351 | time 58[s] | loss 0.00\n","| epoch 8 |  iter 101 / 351 | time 72[s] | loss 0.00\n","| epoch 8 |  iter 121 / 351 | time 86[s] | loss 0.00\n","| epoch 8 |  iter 141 / 351 | time 101[s] | loss 0.00\n","| epoch 8 |  iter 161 / 351 | time 115[s] | loss 0.00\n","| epoch 8 |  iter 181 / 351 | time 129[s] | loss 0.00\n","| epoch 8 |  iter 201 / 351 | time 144[s] | loss 0.00\n","| epoch 8 |  iter 221 / 351 | time 158[s] | loss 0.00\n","| epoch 8 |  iter 241 / 351 | time 172[s] | loss 0.00\n","| epoch 8 |  iter 261 / 351 | time 187[s] | loss 0.00\n","| epoch 8 |  iter 281 / 351 | time 201[s] | loss 0.00\n","| epoch 8 |  iter 301 / 351 | time 215[s] | loss 0.00\n","| epoch 8 |  iter 321 / 351 | time 230[s] | loss 0.00\n","| epoch 8 |  iter 341 / 351 | time 244[s] | loss 0.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 100.000%\n","| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n","| epoch 9 |  iter 21 / 351 | time 14[s] | loss 0.00\n","| epoch 9 |  iter 41 / 351 | time 29[s] | loss 0.00\n","| epoch 9 |  iter 61 / 351 | time 43[s] | loss 0.00\n","| epoch 9 |  iter 81 / 351 | time 57[s] | loss 0.00\n","| epoch 9 |  iter 101 / 351 | time 72[s] | loss 0.00\n","| epoch 9 |  iter 121 / 351 | time 86[s] | loss 0.00\n","| epoch 9 |  iter 141 / 351 | time 100[s] | loss 0.00\n","| epoch 9 |  iter 161 / 351 | time 115[s] | loss 0.00\n","| epoch 9 |  iter 181 / 351 | time 129[s] | loss 0.00\n","| epoch 9 |  iter 201 / 351 | time 143[s] | loss 0.00\n","| epoch 9 |  iter 221 / 351 | time 158[s] | loss 0.00\n","| epoch 9 |  iter 241 / 351 | time 172[s] | loss 0.00\n","| epoch 9 |  iter 261 / 351 | time 186[s] | loss 0.00\n","| epoch 9 |  iter 281 / 351 | time 200[s] | loss 0.00\n","| epoch 9 |  iter 301 / 351 | time 215[s] | loss 0.00\n","| epoch 9 |  iter 321 / 351 | time 229[s] | loss 0.00\n","| epoch 9 |  iter 341 / 351 | time 244[s] | loss 0.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 100.000%\n","| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n","| epoch 10 |  iter 21 / 351 | time 15[s] | loss 0.00\n","| epoch 10 |  iter 41 / 351 | time 29[s] | loss 0.00\n","| epoch 10 |  iter 61 / 351 | time 43[s] | loss 0.00\n","| epoch 10 |  iter 81 / 351 | time 58[s] | loss 0.00\n","| epoch 10 |  iter 101 / 351 | time 72[s] | loss 0.00\n","| epoch 10 |  iter 121 / 351 | time 86[s] | loss 0.00\n","| epoch 10 |  iter 141 / 351 | time 101[s] | loss 0.00\n","| epoch 10 |  iter 161 / 351 | time 115[s] | loss 0.00\n","| epoch 10 |  iter 181 / 351 | time 130[s] | loss 0.00\n","| epoch 10 |  iter 201 / 351 | time 144[s] | loss 0.00\n","| epoch 10 |  iter 221 / 351 | time 159[s] | loss 0.00\n","| epoch 10 |  iter 241 / 351 | time 173[s] | loss 0.00\n","| epoch 10 |  iter 261 / 351 | time 187[s] | loss 0.00\n","| epoch 10 |  iter 281 / 351 | time 202[s] | loss 0.00\n","| epoch 10 |  iter 301 / 351 | time 216[s] | loss 0.00\n","| epoch 10 |  iter 321 / 351 | time 231[s] | loss 0.00\n","| epoch 10 |  iter 341 / 351 | time 245[s] | loss 0.00\n","Q 10/15/94                     \n","T 1994-10-15\n","\u001b[92m☑\u001b[0m 1994-10-15\n","---\n","Q thursday, november 13, 2008  \n","T 2008-11-13\n","\u001b[92m☑\u001b[0m 2008-11-13\n","---\n","Q Mar 25, 2003                 \n","T 2003-03-25\n","\u001b[92m☑\u001b[0m 2003-03-25\n","---\n","Q Tuesday, November 22, 2016   \n","T 2016-11-22\n","\u001b[92m☑\u001b[0m 2016-11-22\n","---\n","Q Saturday, July 18, 1970      \n","T 1970-07-18\n","\u001b[92m☑\u001b[0m 1970-07-18\n","---\n","Q october 6, 1992              \n","T 1992-10-06\n","\u001b[92m☑\u001b[0m 1992-10-06\n","---\n","Q 8/23/08                      \n","T 2008-08-23\n","\u001b[92m☑\u001b[0m 2008-08-23\n","---\n","Q 8/30/07                      \n","T 2007-08-30\n","\u001b[92m☑\u001b[0m 2007-08-30\n","---\n","Q 10/28/13                     \n","T 2013-10-28\n","\u001b[92m☑\u001b[0m 2013-10-28\n","---\n","Q sunday, november 6, 2016     \n","T 2016-11-06\n","\u001b[92m☑\u001b[0m 2016-11-06\n","---\n","val acc 100.000%\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xt0lPWB//HPZCYXcr/NBMI1BAEJ\ngobbsVhoXWj36La/U10lv5Z6a117qj1tV/aszfYUb0FtrbqrPbu01T0eBaVqtmvXKq39yWo1Jpho\ngASSABISwGQmhITcM5nn90fCyCWBCcwzT2bm/TqHwzwzyTOffI185vk+M9/HZhiGIQAAEDZirA4A\nAADGh/IGACDMUN4AAIQZyhsAgDBDeQMAEGYobwAAwozD6gCBcrtPBnV/GRmJam/vCeo+MTrGOjQY\n59BgnEODcR7mdKaMen/UHnk7HHarI0QNxjo0GOfQYJxDg3E+v6gtbwAAwhXlDQBAmKG8AQAIM5Q3\nAABhhvIGACDMUN4AAIQZyhsAgDBDeQMAEGZMXWGtvr5e3//+93Xbbbdp/fr1Zzz2wQcf6IknnpDd\nbteqVat09913mxkFAKJOeW2L3ig7pKOeHuVmJ+r6q2dpxYIcq2Odlz9zW49ys8IscwjH2bTy7unp\n0UMPPaSrr7561McffvhhPfvss8rJydH69ev11a9+VXPmzDErDizA/4ShwThjNOW1Ldr8eo1/u9nd\n7d+eqGNN5sCZVt5xcXH6zW9+o9/85jfnPNbU1KS0tDRNmTJFkrR69WqVlZVR3hFkrF9oz4leLZyd\nZWGyse052KbX3j3o3z6V2X2iVwtnZ0qSDGP07z39fkNnfZEx6s2z7v98Y6znGH7szAdrD7XrDx8c\nOifzEU+3Lp+Zcc4OT//usbKcnWes/OfmHP1nOPt56ptO6K2Kw+dk7uju14rLcxQXa1d8nF0xNtvZ\nT4CzGIahgUGf+ga86hscUl//kPoGvOofHNIrO/aP+j0v/6VBbZ19IU4amD/vbBr1/nDM/EZZY3iW\nt8PhkMMx+u7dbrcyMzP925mZmWpqGn0ATsnISAz6WrdjLfiOS7d950ej3v/auwfPKMhwUPruQZWG\nWeb/+eCQ/ue0Ug8HL/9lv17+y+eFEx9nV0KcXQlxDk2Kd/hvJ8TblRDvGL4dZx95bOT+OIcmxdsV\nH+fQpJH7JsU7FB9n16S44b9tF/mi4N2Pm/XKXxp0uOWkZuSk6Ka/uUyrrpo2rn34fIb6Brzq7T/z\nT1//kHr8t0d/fPj2oHr9t73qG/Ce98XeaDq6B/TqjgPj+yaLhWPmY23dpnZM2FxVLNhXl3E6U4J+\npTJ87vBno4+tzSatWTI9xGkC8/ZHTWcfgEoazrx26fQztv23dcbGaDdHeezzjbF65Nz7baPcGi7p\nsTJ/fWXe6HnGynnWk9rGfui0+8f+nrF+5ld27B+1cGySll3uUt/AkPoHhtQ/OPynb2BIJ7sH1D84\npCHfOJtqlOc4dWSfMPJ3/GnbcbHDLxjO3m52d+n/VR3x7+fQsU794sVKlVUf0ZSsJPUOjBzxDgzn\nHf4ZvMO3B0/d59XAoO+S8sfFxighdvhFijMtYeQFjuO0Fzqfv+B5+6MmnegaOGcf2WkJWv+VeZeU\nwywv/qlOno5zj7DDMfOUrKSgdMxYLwAsKW+XyyWPx+PfbmlpkcvlsiIKTJKbnahmd/c590/NTtb/\nXXOZBYkubG/j8TEzF/3NxMz8cYN7zMz/55q8Ub7Deh/sOTZ6Zmeyvvd/Fp73e71DPvUNDGlgpBD7\nT/u7/7S/+05tn/qaUR/36mTvoPoHhuQb7+HriL/u/mzMx2wanj2IH5kdyEiJHyne4ZmD+Ngzy9b/\nouG07VO3T31tTEzgswZZqQlnnLo65cbV+VqUPzFPXd24Oj9iMl9/9UxTn9eS8p42bZq6urrU3Nys\nyZMn65133tHjjz9uRRSYZMk8l5rdn55zv9m/0Jfi+qtnWfI/4aWItswOe4ySJ8VIk2KDlscwDHmH\nfOofOXd8qtwHTnsR8Jv/qR11tiDGJv3gxkXnFG583PBRu5Xn7U+db32jrFHH2ro1JStJ1189c8K+\n8Usi83jYjLPfARMke/bs0WOPPaYjR47I4XAoJydH1157raZNm6a1a9dq586d/sL+yle+ou985zvn\n3V+wp7iZNjfX1j/X6+3KZmWnJaj9ZH9Y/E8onXoXdPj8wyGRORR+9mz5qLMF05zJevA7yy1IFPn4\nN3rYWNPmppV3sFHe4cPnM3Tvr96Xd8inJ39wjaZMTmOsQ4DfafOc/emJU+76esGEftERzvh9Hjah\nznkjstU1nVBH94BWX5krh51F/BD+wnE6F5GN8kbQVextkSQtv5x/2BA5VizI0YoFORwRYkLgsAhB\n5R3y6aN9rUpLitO86elWxwGAiER5I6hqD7Wru8+rZfNd4/pYCwAgcJQ3gso/Zc65QAAwDeWNoBn0\nDqmq3q2s1ATl56ZaHQcAIhbljaDZdeC4+gaGtPxy10WvHw0AuDDKG0HDu8wBIDQobwRF34BX1fs9\nyslM1IycZKvjAEBEo7wRFJ/s92jA69MKpswBwHSUN4KiorZVkrSMKXMAMB3ljUvW3Teo3QfbNM2Z\nrKnZSVbHAYCIR3njklXVuzXkM7RiAddkB4BQoLxxySr2MmUOAKFEeeOSdHYPaO+hduVNSZUrfZLV\ncQAgKlDeuCSVda3yGYZWXM6UOQCECuWNS1K+t1U2MWUOAKFEeeOiHe/sU0PTCV02PV0ZKfFWxwGA\nqEF546J9tK9VhsSUOQCEGOWNi1a+t1UxNpuWzKO8ASCUKG9clNYTvfr0WKcun5Wh1KQ4q+MAQFSh\nvHFRdvqvIMZRNwCEGuWNi1Je2yp7jE1L5jqtjgIAUYfyxrgd8XSr2d2lK2ZnKTEh1uo4ABB1KG+M\nm3/KnLXMAcASlDfGxTAMle9tVZwjRlfOybY6DgBEJcob43K4pUstx3u0eE62EuIcVscBgKhEeWNc\nKvzvMmc5VACwCuWNgBmGoYq9LUqIs2tRfqbVcQAgalHeCNiBo51q6+xX4VynYh12q+MAQNSivBGw\nilqmzAFgIqC8ERCfz9DOfa1KSnBowawMq+MAQFSjvBGQuqYT6uge0NL5Ljns/NoAgJX4VxgB4V3m\nADBxUN64IO+QTx/ta1VaUpzmTU+3Og4ARD3KGxdUe6hd3X1eLZvvUkyMzeo4ABD1KG9ckH/KfAFT\n5gAwEVDeOK9B75Cq6t3KSk1Qfm6q1XEAAKK8cQG7DrSpb2BIyy93yWZjyhwAJgLKG+dVvrdVEu8y\nB4CJhPLGmHr7vdq136OczETNyEm2Og4AYATljTFV7/dowOvTCqbMAWBCobwxpoqRKfNlTJkDwIRC\neWNU3X2D2n2wTdOcyZqanWR1HADAaShvjKqqzq0hn6EVC1xWRwEAnMVh5s43bdqk6upq2Ww2FRcX\na9GiRf7HtmzZotdff10xMTFauHCh/uVf/sXMKBinUwuzMGUOABOPaUfeFRUVamxs1LZt21RSUqKS\nkhL/Y11dXXr22We1ZcsWvfTSSzpw4IA++eQTs6JgnDq7B1Tb2K68KalypU+yOg4A4CymlXdZWZnW\nrFkjScrPz1dHR4e6urokSbGxsYqNjVVPT4+8Xq96e3uVlpZmVhSM00d1rTIMacXlTJkDwERkWnl7\nPB5lZGT4tzMzM+V2uyVJ8fHxuvvuu7VmzRp9+ctf1uLFi5WXl2dWFIxTRW2LbGLKHAAmKlPPeZ/O\nMAz/7a6uLm3evFlvvfWWkpOTdeutt2rfvn2aP3/+mN+fkZEoh8Me1ExOZ0pQ9xcJPCd6Vd/coYLZ\nWZo7Ozto+2WsQ4NxDg3GOTQY57GZVt4ul0sej8e/3draKqfTKUk6cOCApk+frszMTEnS0qVLtWfP\nnvOWd3t7T1DzOZ0pcrtPBnWfkWB7xWFJUuGcrKCND2MdGoxzaDDOocE4DxvrBYxp0+YrV67U9u3b\nJUk1NTVyuVxKTh5eYnPq1Kk6cOCA+vr6JEl79uzRrFmzzIqCcajY26IYm01L5nG+GwAmKtOOvAsL\nC1VQUKCioiLZbDZt3LhRpaWlSklJ0dq1a/Wd73xHt9xyi+x2u6666iotXbrUrCgIUGt7jz49dlIF\neZlKTYqzOg4AYAymnvPesGHDGdunT4sXFRWpqKjIzKfHOFX4ryDGUTcATGSssAa/ir0tssfYtGSu\n0+ooAIDzoLwhSTri7lKzu1tXzM5SYkKs1XEAAOdBeUPSaVPmrGUOABMe5Q0ZhqGKvS2Kc8ToyjnB\n+2w3AMAclDd0uKVLLe29WjwnWwlxIVu3BwBwkShvqHzkCmLLWQ4VAMIC5R3lfIahnXtblBBn16L8\nTKvjAAACQHlHuYNHOtXW2a/CuU7FBnnteACAOSjvKMeUOQCEH8o7ivl8hnbua1VSgkMLZmVc+BsA\nABMC5R3F6g63q7N7QEvnu+Sw86sAAOGCf7GjWLl/LXOmzAEgnFDeUco75FNlXavSkuI0b3q61XEA\nAONAeUep2kPH1d3n1bL5LsXE2KyOAwAYB8o7SpXXnlrLnClzAAg3lHcUGhgc0scNbmWlJig/N9Xq\nOACAcaK8o9Dug23qGxjS8stdstmYMgeAcEN5RyHeZQ4A4Y3yjjK9/V7t2u9RTmaiZuQkWx0HAHAR\nKO8oU73fowGvTyuYMgeAsEV5R5mKkSnzZUyZA0DYoryjSHffoHYfbNM0Z7KmZidZHQcAcJEo7yhS\nVefWkM/QigUuq6MAAC4B5R1FKkYu/8mUOQCEN8o7SnR2D6i2sV15U1LlSp9kdRwAwCWgvKPER3Wt\nMgxpxeVMmQNAuKO8o0RFbYtsYsocACIB5R0Fjnf2qb65Q5dNT1dGSrzVcQAAl4jyjgI79w1/tpsp\ncwCIDJR3FKjY26IYm01L5lHeABAJKO8I19reo0+PndTlszKUmhRndRwAQBBQ3hGuwn8FMY66ASBS\nUN4RrmJvi+wxNi2Z67Q6CgAgSCjvCHbE3aVmd7eumJ2lxIRYq+MAAIKE8o5g/ilz1jIHgIhCeUco\nwzBUsbdFcY4YXTkn2+o4AIAgorwj1OGWLrW092rxnGwlxDmsjgMACCLKO0KVj1xBbDnLoQJAxKG8\nI5DPMLRzb4sS4uxalJ9pdRwAQJBR3hHo4JFOtXX2q3CuU7EOu9VxAABBRnlHIKbMASCyUd4Rxucz\ntHNfq5ISHFowK8PqOAAAE1DeEabucLs6uwe0dL5LDjv/eQEgEvGve4Qp969lzpQ5AEQqyjuCeId8\nqqxrVVpSnOZNT7c6DgDAJJR3BKk9dFzdfV4tm+9STIzN6jgAAJOYuvTWpk2bVF1dLZvNpuLiYi1a\ntMj/2LFjx/SP//iPGhwc1IIFC/Tggw+aGSUqlNeeWsucKXMAiGSmHXlXVFSosbFR27ZtU0lJiUpK\nSs54/NFHH9Udd9yhV199VXa7XUePHjUrSlQYGBzSxw1uZaUmKD831eo4AAATmVbeZWVlWrNmjSQp\nPz9fHR0d6urqkiT5fD5VVlbq2muvlSRt3LhRubm5ZkWJCrsPtqlvYEjLL3fJZmPKHAAimWnT5h6P\nRwUFBf7tzMxMud1uJScn6/jx40pKStIjjzyimpoaLV26VPfee+9595eRkShHkFcLczpTgro/K33y\n5j5J0le/kDchf66JmCkSMc6hwTiHBuM8tpBdbsowjDNut7S06JZbbtHUqVP1D//wD9qxY4e+9KUv\njfn97e09Qc3jdKbI7T4Z1H1apbffq501nyknM1EpcTET7ueKpLGeyBjn0GCcQ4NxHjbWCxjTps1d\nLpc8Ho9/u7W1VU6nU5KUkZGh3NxczZgxQ3a7XVdffbUaGhrMihLxqvd7NOD1aQVT5gAQFUwr75Ur\nV2r79u2SpJqaGrlcLiUnJ0uSHA6Hpk+frkOHDvkfz8vLMytKxKsYWZhlGQuzAEBUMG3avLCwUAUF\nBSoqKpLNZtPGjRtVWlqqlJQUrV27VsXFxbrvvvtkGIbmzp3rf/Maxqe7b1C7D7ZpmjNZU7OTrI4D\nAAgBU895b9iw4Yzt+fPn+2/PnDlTL730kplPHxWq6twa8hlascBldRQAQIiwwlqYqxi5/CdT5gAQ\nPQIq79PfKY6Jo7N7QLWN7cqbkipX+iSr4wAAQiSg8v7yl7+sJ598Uk1NTWbnwTh8VNcqw5BWXM6U\nOQBEk4DK+5VXXpHT6VRxcbFuv/12/eEPf9DAwIDZ2XABFbUtsokpcwCINgGVt9Pp1Pr16/XCCy/o\n/vvv10svvaQvfvGLevLJJ9Xf3292RozieGef6ps7dNn0dGWkxFsdBwAQQgG/YW3nzp36yU9+ojvv\nvFOFhYXaunWrUlNT9cMf/tDMfBjDzn3Dn+1myhwAok9AHxVbu3atpk6dqptvvlkPPvigYmNjJQ1f\ncOTtt982NSBGV7G3RTE2m5bMo7wBINoEVN6//e1vZRiGZs2aJUmqra3VggULJElbt241LRxG19re\no0+PnVRBXqZSk+KsjgMACLGAps1LS0u1efNm//avf/1rPf7445LEWtoWOLUc6nKmzAEgKgVU3uXl\n5XrkkUf820899ZQqKytNC4Xzq9jbInuMTUvmOq2OAgCwQEDlPTg4eMZHw7q7u+X1ek0LhbEdcXep\n2d2tK2ZnKTEh1uo4AAALBHTOu6ioSNddd50WLlwon8+n3bt365577jE7G0bhnzJnLXMAiFoBlfdN\nN92klStXavfu3bLZbPrJT37iv7wnQscwDFXsbVGcI0ZXzsm2Og4AwCIBf867p6dHmZmZysjI0MGD\nB3XzzTebmQujONzSpZb2Xi2ek62EOFMvCAcAmMACaoCHH35Y77//vjwej2bMmKGmpibdcccdZmfD\nacprW7Tlz/WSpINHO1Ve26IVC1gWFQCiUUBH3rt379abb76p+fPn67XXXtNzzz2n3t5es7NhRHlt\niza/XqOu3kFJUltnnza/XqPy2haLkwEArBBQecfFDS8EMjg4KMMwtHDhQlVVVZkaDJ97o+zQGPc3\nhjQHAGBiCGjaPC8vT1u2bNHSpUt1++23Ky8vTydPnjQ7G0Yc9fSMev+xtu4QJwEATAQBlfcDDzyg\njo4Opaam6o033lBbW5vuuusus7NhRG52oprd5xb1lKwkC9IAAKwW0LT5pk2blJ6erpiYGH3ta1/T\nbbfdpsmTJ5udDSNWjHG97uuvnhniJACAiSCg8rbb7SorK1N/f798Pp//D0Kj3zs81lmpCbLH2DTN\nmay7vl7Au80BIEoFNG3+yiuv6Pnnn5dhGP77bDab9u7da1owfK6q3q1YR4we+u5yPt8NAAisvLkI\niXWOtXXrqKdbV13GwiwAgGEBtcG//uu/jnr/D3/4w6CGwbkq69ySpEKuIAYAGBHwOe9Tf3w+n8rL\ny/moWIhU1rtlj7HpystYyxwAMCygI++zryA2NDSkH/zgB6YEwuc8Hb1q/OykCvIylcTlPwEAIwK+\nMMnpvF6vDh8+HOwsOEvVyJT5EqbMAQCnCejIe/Xq1bLZbP7tjo4OfeMb3zAtFIZV1rtlk3QVU+YA\ngNMEVN5bt27137bZbEpOTlZqaqppoSB1dPVrf3OHLpuWprTkeKvjAAAmkICmzXt7e/Xyyy9r6tSp\nys3N1SOPPKKGhgazs0W1jxs8MiQVznNZHQUAMMEEVN4PPPCAVq9e7d++8cYb9eCDD5oWClJlXask\nqXAuU+YAgDMFVN5DQ0NaunSpf3vp0qVnrLaG4OrqHdS+wyc0c3KKstMmWR0HADDBBHTOOyUlRVu3\nbtWKFSvk8/n03nvvKSmJK1qZpXq/R0M+Q0vn8S5zAMC5AirvRx55RL/85S/10ksvSZIKCwv1yCOP\nmBosmlXVs6oaAGBsAZV3Zmam7rzzTs2aNUuSVFtbq8zMTDNzRa2+Aa/2fHpcudlJXK8bADCqgM55\nP/nkk9q8ebN/+9e//rUef/xx00JFs90Hj2vQ6+OoGwAwpoDKu7y8/Ixp8qeeeoorjZnk1LvMOd8N\nABhLQOU9ODiogYEB/3Z3d7e8Xq9poaLVoHdI1QfalJ2WoOmuZKvjAAAmqIDOeRcVFem6667TwoUL\n5fP5tHv3bt16661mZ4s6NYfa1T8wpC9dmXvGcrQAAJwuoPK+6aabNGvWLLW3t8tms+naa6/V5s2b\nddttt5kcL7p8fiESVlUDAIwtoPIuKSnRX//6V3k8Hs2YMUNNTU264447zM4WVYZ8Pn3c4FZacpxm\nT2XdeADA2AI6571r1y69+eabmj9/vl577TU999xz6u3tNTtbVKk/fELdfV4VznUqhilzAMB5BFTe\ncXFxkobfuGYYhhYuXKiqqipTg0Wbj+q5djcAIDABTZvn5eVpy5YtWrp0qW6//Xbl5eXp5MmTZmeL\nGj7DUFW9W0kJDs2dnm51HADABBdQeT/wwAPq6OhQamqq3njjDbW1temuu+4yO1vUOHi0Ux1dA7rm\niily2AOaDAEARLGAyttmsyk9ffiI8Gtf+1rAO9+0aZOqq6tls9lUXFysRYsWnfM1v/zlL/XJJ5/o\nhRdeCHi/kebUu8wLWZgFABAA0w7zKioq1NjYqG3btqmkpEQlJSXnfM3+/fu1c+dOsyKEBcMw9FFd\nq+Lj7CqYlWF1HABAGDCtvMvKyrRmzRpJUn5+vjo6OtTV1XXG1zz66KP68Y9/bFaEsNDU2iVPR58W\n52cp1mG3Og4AIAwENG1+MTwejwoKCvzbmZmZcrvdSk4eXvaztLRUy5cv19SpUwPaX0ZGohxBLjen\nMyWo+7sY2yubJUlfXjZjQuQxSyT/bBMJ4xwajHNoMM5jM628z2YYhv/2iRMnVFpaqv/8z/9US0tL\nQN/f3t4T1DxOZ4rcbuvfMf/Xj4/IYY/RzOzECZHHDBNlrCMd4xwajHNoMM7DxnoBY9q0ucvlksfj\n8W+3trbK6Rx+Q9aHH36o48eP61vf+pbuuece1dTUaNOmTWZFmbCOtXXriKdbC/MylRAXstdRAIAw\nZ1p5r1y5Utu3b5ck1dTUyOVy+afM//Zv/1Z//OMf9bvf/U7PPPOMCgoKVFxcbFaUCavq1MIsvMsc\nADAOph3uFRYWqqCgQEVFRbLZbNq4caNKS0uVkpKitWvXmvW0YaWyzi17jE2L52RbHQUAEEZMnavd\nsGHDGdvz588/52umTZsWlZ/xbuvo06HPTqpgVoaSJ8VaHQcAEEZYzssip6bMC+dx+U8AwPhQ3hap\nrGuVTdJVlzFlDgAYH8rbAh3dA2po7lD+tDSlJ8dbHQcAEGYobwt83OCWIWkpl/8EAFwEytsC/guR\nUN4AgItAeYdYd9+g9ja2a2ZOirLTJ1kdBwAQhijvEKve79GQz+DynwCAi0Z5h1jlyJT5UsobAHCR\nKO8Q6h8Y0p5Pj2tKVqKmZCVZHQcAEKYo7xDafbBNg14fa5kDAC4J5R1ClacuRDKXVdUAABeP8g6R\nQa9P1fs9yk5L0IycZKvjAADCGOUdInsbj6tvYEiFc52y2WxWxwEAhDHKO0Q+quPa3QCA4KC8Q2DI\n59MnDR6lJcUpf2qa1XEAAGGO8g6B+qYOdfUOqnCuUzFMmQMALhHlHQL+tcyZMgcABAHlbTKfYaiy\nvlVJCQ7Nm55udRwAQASgvE326dFOnega0JVzsuWwM9wAgEtHm5jMvzDLPBZmAQAEB+VtIsMwVFXn\nVnysXQV5GVbHAQBECMrbRE2tXWo90atF+VmKdditjgMAiBCUt4mq6lmYBQAQfJS3iSrr3XLYY3TF\n7CyrowAAIgjlbZLPjvfoiLtbC/MyNSneYXUcAEAEobxNUlnXKkkqnMuUOQAguChvk1TVuxVjs+nK\ny7KtjgIAiDCUtwnaOvr06bGTmj8zXcmTYq2OAwCIMJS3CaoaRt5lzpQ5AMAElLcJKuvcskm6ivIG\nAJiA8g6yju4BNTSdUP7UNKUnx1sdBwAQgSjvIPukwS1DLMwCADAP5R1kpy5EwkfEAABmobyDqKdv\nUHsPtWtGTrKc6ZOsjgMAiFCUdxBV72/TkM/gXeYAAFNR3kHEtbsBAKFAeQdJ/8CQ9hxs05SsROVm\nJ1kdBwAQwSjvINnzaZsGvD7eqAYAMB3lHSSVdVy7GwAQGpR3EAx6fao+4FFWaoJm5qRYHQcAEOEo\n7yDY29iu3v4hLZnnlM1mszoOACDCUd5BUFXPtbsBAKFDeV+iIZ9PVfUepSbFac7UNKvjAACiAOV9\niRqaOtTVO6jCuU7FxDBlDgAwH+V9ifwLszBlDgAIEYeZO9+0aZOqq6tls9lUXFysRYsW+R/78MMP\n9cQTTygmJkZ5eXkqKSlRTEx4vZbwGYaq6t1KjHdo3ox0q+MAAKKEaW1ZUVGhxsZGbdu2TSUlJSop\nKTnj8Z/97Gf6t3/7N7388svq7u7We++9Z1YU03x6rFPtJ/t15WXZctjD64UHACB8mdY4ZWVlWrNm\njSQpPz9fHR0d6urq8j9eWlqqyZMnS5IyMzPV3t5uVhTTVLEwCwDAAqaVt8fjUUZGhn87MzNTbrfb\nv52cnCxJam1t1fvvv6/Vq1ebFcUUhmGost6t+Fi7CmZlWh0HABBFTD3nfTrDMM65r62tTd/73ve0\ncePGM4p+NBkZiXI47EHN5HRe/Gpoh451qrW9VysX52pqLue7L+RSxhqBY5xDg3EODcZ5bKaVt8vl\nksfj8W+3trbK6fx8ermrq0t33nmnfvSjH+maa6654P7a23uCms/pTJHbffKiv//PZZ9KkhbOzLik\n/USDSx1rBIZxDg3GOTQY52FjvYAxbdp85cqV2r59uySppqZGLpfLP1UuSY8++qhuvfVWrVq1yqwI\npqqqd8tht2lRfpbVUQAAUca0I+/CwkIVFBSoqKhINptNGzduVGlpqVJSUnTNNdfo97//vRobG/Xq\nq69Kkv7u7/5O69atMytOULUc71Gzu1uL87M0KT5kZx4AAJBk8jnvDRs2nLE9f/58/+09e/aY+dSm\nqhpZmKWQd5kDACzAh5Mvwkd1bsXYbLrqMsobABB6lPc4He/s06fHOjVvRrqSJ8VaHQcAEIUo73E6\nNWXOwiwAAKtQ3uN0qryZMge9sra2AAAL3klEQVQAWIXyHofOngHVNZ1Q/tRUZaTEWx0HABClKO9x\n+KTBI8OQlsx1WR0FABDFKO9xqKzjI2IAAOtR3gHq6fOq9tBxzXAly5U+yeo4AIAoRnkHqPqAR0M+\ng6NuAIDlKO8A+a/dPZfyBgBYi/IOQP/gkHYfbNPkzETlZidZHQcAEOUo7wDsOXhcA16flsxzymaz\nWR0HABDlKO8AVNa3SpIKmTIHAEwAlPcFeId8qt7vUVZqvGZNHv2i6AAAhBLlfQF7G9vV2z+kwrku\npswBABMC5X0BpxZm4UIkAICJgvI+D5/P0McNbqUmxmrO1DSr4wAAIInyPq+G5hM62TOoq+Y6FRPD\nlDkAYGKgvM+DKXMAwEREeY/BMAxV1ruVGO/Q/BkZVscBAMCP8h7Doc9Oqv1kvxbPyZbDzjABACYO\nWmkMH9UNL8zClDkAYKKhvEdhGIYq69yKi43RwrxMq+MAAHAGynsURzzdam3v1aLZWYqLtVsdBwCA\nM1Deozh1+U+u3Q0AmIgo71F8VOeWw27T4vxsq6MAAHAOyvssLe09anZ3acGsTE2Kd1gdBwCAc1De\nZ6mqH1mYhct/AgAmKMr7LFV1btls0pWXMWUOAJiYKO/THO/s04GjnZo3PV0piXFWxwEAYFSU92k+\nbvBIkpbMc1mcBACAsVHep6kcWVWtkPPdAIAJjPIecbJnQHVNJ5Sfm6qMlHir4wAAMCbKe8THDR4Z\nBguzAAAmPsp7BB8RAwCEC8pbUk+fV7WHjmu6K1mujESr4wAAcF6Ut6RdBz3yDhkcdQMAwgLlLamS\nC5EAAMJI1Jd3/+CQdh9sU05moqZmJ1kdBwCAC4r68q759LgGBn1aMtcpm81mdRwAAC4o6sv71JT5\nEqbMAQBhIqrL2zvk0yf7PcpMjdesySlWxwEAICBRXd77GtvV2+9VIVPmAIAwEtXlXcnCLACAMBS1\n5T3kM/RxvVspibG6bFq61XEAAAhY1Jb33k/b1NkzqKsucyomhilzAED4cJi5802bNqm6ulo2m03F\nxcVatGiR/7EPPvhATzzxhOx2u1atWqW7777bzCh+5bUteqPskI64uyVJyZNMHQIAAILOtCPviooK\nNTY2atu2bSopKVFJSckZjz/88MN6+umn9dJLL+n999/X/v37zYriV17bos2v16jZ3S1j5L4/fnhY\n5bUtpj83AADBYlp5l5WVac2aNZKk/Px8dXR0qKurS5LU1NSktLQ0TZkyRTExMVq9erXKysrMiuL3\nRtmhMe5vNP25AQAIFtPmjD0ejwoKCvzbmZmZcrvdSk5OltvtVmZm5hmPNTU1nXd/GRmJcjjsl5Tp\naFvPqPcfa+uW08nnvM3E+IYG4xwajHNoMM5jC9kJX8MwLvxF59HePnrxjkduVqKaR851n25KVpLc\n7pOXvH+MzulMYXxDgHEODcY5NBjnYWO9gDFt2tzlcsnj8fi3W1tb5XQ6R32spaVFLpfLrCh+1189\na4z7Z5r+3AAABItp5b1y5Upt375dklRTUyOXy6Xk5GRJ0rRp09TV1aXm5mZ5vV698847WrlypVlR\n/FYsyNFdXy/QNGey7DE2TXMm666vF2jFghzTnxsAgGAxbdq8sLBQBQUFKioqks1m08aNG1VaWqqU\nlBStXbtW999/v+69915J0nXXXae8vDyzopxhxYIcrViQw5QMACBs2YxLPRkdIsEuWso7dBjr0GCc\nQ4NxDg3GeVjIz3kDAABzUN4AAIQZyhsAgDBDeQMAEGYobwAAwgzlDQBAmKG8AQAIM5Q3AABhJmwW\naQEAAMM48gYAIMxQ3gAAhBnKGwCAMEN5AwAQZihvAADCDOUNAECYicry3rRpk9atW6eioiLt2rXL\n6jgR6+c//7nWrVunG2+8UX/605+sjhPR+vr6tGbNGpWWllodJaK9/vrr+vrXv64bbrhBO3bssDpO\nROru7tY999yjb3/72yoqKtJ7771ndaQJyWF1gFCrqKhQY2Ojtm3bpgMHDqi4uFjbtm2zOlbE+fDD\nD9XQ0KBt27apvb1d3/jGN/SVr3zF6lgR69///d+VlpZmdYyI1t7erl/96ld67bXX1NPTo6efflpf\n+tKXrI4Vcf7rv/5LeXl5uvfee9XS0qJbb71Vb731ltWxJpyoK++ysjKtWbNGkpSfn6+Ojg51dXUp\nOTnZ4mSRZdmyZVq0aJEkKTU1Vb29vRoaGpLdbrc4WeQ5cOCA9u/fT5GYrKysTFdffbWSk5OVnJys\nhx56yOpIESkjI0N1dXWSpM7OTmVkZFicaGKKumlzj8dzxi9DZmam3G63hYkik91uV2JioiTp1Vdf\n1apVqyhukzz22GO67777rI4R8Zqbm9XX16fvfe97+uY3v6mysjKrI0Wk66+/XkePHtXatWu1fv16\n/fM//7PVkSakqDvyPhurw5rr7bff1quvvqrnnnvO6igR6fe//72uvPJKTZ8+3eooUeHEiRN65pln\ndPToUd1yyy165513ZLPZrI4VUf77v/9bubm5evbZZ7Vv3z4VFxfzXo5RRF15u1wueTwe/3Zra6uc\nTqeFiSLXe++9p//4j//Qb3/7W6WkpFgdJyLt2LFDTU1N2rFjhz777DPFxcVp8uTJ+sIXvmB1tIiT\nlZWlq666Sg6HQzNmzFBSUpKOHz+urKwsq6NFlKqqKl1zzTWSpPnz56u1tZVTbqOIumnzlStXavv2\n7ZKkmpoauVwuzneb4OTJk/r5z3+uzZs3Kz093eo4Eeupp57Sa6+9pt/97ne66aab9P3vf5/iNsk1\n11yjDz/8UD6fT+3t7erp6eF8rAlmzpyp6upqSdKRI0eUlJREcY8i6o68CwsLVVBQoKKiItlsNm3c\nuNHqSBHpj3/8o9rb2/WjH/3If99jjz2m3NxcC1MBFy8nJ0df/epXdfPNN0uSfvrTnyomJuqOf0y3\nbt06FRcXa/369fJ6vbr//vutjjQhcUlQAADCDC8bAQAIM5Q3AABhhvIGACDMUN4AAIQZyhsAgDBD\neQO4KKWlpdqwYYPVMYCoRHkDABBmom6RFiDavPDCC3rzzTc1NDSk2bNn67vf/a7uuusurVq1Svv2\n7ZMkPfnkk8rJydGOHTv0q1/9SgkJCZo0aZIeeugh5eTkqLq6Wps2bVJsbKzS0tL02GOPSZK6urq0\nYcMGHThwQLm5uXrmmWfU2trqPyLv6+vTunXr9Pd///eW/fxAJOLIG4hgu3bt0p///Gdt2bJF27Zt\nU0pKij744AM1NTXphhtu0NatW7V8+XI999xz6u3t1U9/+lM9/fTTeuGFF7Rq1So99dRTkqR/+qd/\n0kMPPaQXX3xRy5Yt0//+7/9Kkvbv36+HHnpIpaWlamhoUE1Njd58803Nnj1bL7zwgl588UX19fVZ\nOQRAROLIG4hg5eXlOnz4sG655RZJUk9Pj1paWpSenq6FCxdKGl4y+Pnnn9ehQ4eUlZWlyZMnS5KW\nL1+ul19+WcePH1dnZ6fmzp0rSbrtttskDZ/zvuKKKzRp0iRJw8uHnjx5Ul/84he1detW3XfffVq9\nerXWrVsX4p8aiHyUNxDB4uLidO211+pnP/uZ/77m5mbdcMMN/m3DMGSz2c65tOXp94+1ivLZF4ww\nDEP5+fl64403tHPnTr311lt6/vnn9fLLLwfxpwLAtDkQwQoLC/Xuu++qu7tbkrRlyxa53W51dHSo\ntrZW0vAlGOfNm6dZs2apra1NR48elSSVlZVp8eLFysjIUHp6unbt2iVJeu6557Rly5Yxn/MPf/iD\ndu/erS984QvauHGjjh07Jq/Xa/JPCkQXjryBCHbFFVfoW9/6lr797W8rPj5eLpdLK1asUE5OjkpL\nS/Xoo4/KMAw98cQTSkhIUElJiX784x8rLi5OiYmJKikpkST94he/0KZNm+RwOJSSkqJf/OIX+tOf\n/jTqc86ZM0cbN25UXFycDMPQnXfeKYeDf2qAYOKqYkCUaW5u1je/+U29++67VkcBcJGYNgcAIMxw\n5A0AQJjhyBsAgDBDeQMAEGYobwAAwgzlDQBAmKG8AQAIM5Q3AABh5v8Dhote8Is8SYIAAAAASUVO\nRK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f5499045eb8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"fYpqeYYaaBne","colab_type":"text"},"cell_type":"markdown","source":["## visualize_attention"]},{"metadata":{"id":"5fIrDLXvaCs9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1667},"outputId":"6a4f39a4-f362-40ab-cee0-c22bb27cdbb6","executionInfo":{"status":"ok","timestamp":1538317410952,"user_tz":-540,"elapsed":2310,"user":{"displayName":"maro amo","photoUrl":"","userId":"13741098860389575110"}}},"cell_type":"code","source":["(x_train, t_train), (x_test, t_test) = load_data('date.txt')\n","char_to_id, id_to_char = get_vocab()\n","\n","# Reverse input\n","x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n","\n","vocab_size = len(char_to_id)\n","wordvec_size = 16\n","hidden_size = 256\n","\n","model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n","model.load_params()\n","\n","_idx = 0\n","def visualize(attention_map, row_labels, column_labels):\n","    fig, ax = plt.subplots()\n","    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n","\n","    ax.patch.set_facecolor('black')\n","    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n","    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n","    ax.invert_yaxis()\n","    ax.set_xticklabels(row_labels, minor=False)\n","    ax.set_yticklabels(column_labels, minor=False)\n","\n","    global _idx\n","    _idx += 1\n","    plt.show()\n","\n","\n","np.random.seed(1984)\n","for _ in range(5):\n","    idx = [np.random.randint(0, len(x_test))]\n","    x = x_test[idx]\n","    t = t_test[idx]\n","\n","    model.forward(x, t)\n","    d = model.decoder.attention.attention_weights\n","    d = np.array(d)\n","    attention_map = d.reshape(d.shape[0], d.shape[2])\n","\n","    # reverse for print\n","    attention_map = attention_map[:,::-1]\n","    x = x[:,::-1]\n","\n","    row_labels = [id_to_char[i] for i in x[0]]\n","    column_labels = [id_to_char[i] for i in t[0]]\n","    column_labels = column_labels[1:]\n","\n","    visualize(attention_map, row_labels, column_labels)"],"execution_count":32,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f5498fa1630>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkAiIgsAiYgsACQisgCQiMgCQCL/A6IWeeA9IOAaAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f5498fa13c8>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f5498f088d0>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f5498da7ef0>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f5498da0cc0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"eR5kmBdHaW70","colab_type":"text"},"cell_type":"markdown","source":["今回もcupyに挑戦してみていましたが、うまくいかず日が回りそうなのでひとまず諦めます  \n","まだまだコーディング力が足りないなと身をしみて感じました。  \n","\n","観察としては、例えばepoch６のところで、  \n","accuracyが、出力される回答は全て正解でも、  \n","確率的には不完全だったのか99%を返しており、  \n","経験の精度、思考の精度を表しているところは深層学習だなぁと感じました。  \n","\n","自然言語の分野で、RNNやAttentionのことを学ぶことをして素直によかったです。  \n","深層学習において様々な分野に応用されている技術なのだなということが理解でき、  \n","やる気なくしていた頃の自分には反省してもらおうと思います。"]}]}